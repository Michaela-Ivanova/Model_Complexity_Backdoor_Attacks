{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_BF9iNg08Rj",
        "outputId": "9d193713-d0f8-4fe3-ce8c-9f9654a98ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "NjVrcNG604JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. MINIMAL CNN - Ultra-simple baseline"
      ],
      "metadata": {
        "id": "rYy6SCCcIHG3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8XLvcaBIAXU"
      },
      "outputs": [],
      "source": [
        "class MinimalCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Very simple 3-layer CNN with minimal parameters\n",
        "    ~15K parameters\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MinimalCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)  # 32x32 -> 32x32\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # 32x32 -> 16x16\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)  # 16x16 -> 16x16\n",
        "        # After second pool: 8x8\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 8x8 -> 8x8\n",
        "        # After third pool: 4x4\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. NARROW DEEP CNN - Small width, more depth"
      ],
      "metadata": {
        "id": "MkGE4SMNIL5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NarrowDeepCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Narrow but deeper network - testing depth vs width\n",
        "    ~25K parameters\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(NarrowDeepCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)  # 16x16\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool(x)  # 8x8\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = self.pool(x)  # 4x4\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "YAUMYPjBIO38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. WIDE SHALLOW CNN - More width, less depth"
      ],
      "metadata": {
        "id": "5_7i09dyISPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WideShallowCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Wide but shallow network - testing width vs depth\n",
        "    ~50K parameters\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(WideShallowCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 16x16\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 8x8\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # 4x4\n",
        "        x = x.view(-1, 256 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "k7HIM2MjIUqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. SMALL KERNEL CNN - Using 1x1 and 3x3 kernels only"
      ],
      "metadata": {
        "id": "GsFaU1FdIZA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SmallKernelCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Using only small kernels (1x1, 3x3) - inspired by modern architectures\n",
        "    ~30K parameters\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SmallKernelCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv1_1x1 = nn.Conv2d(32, 32, kernel_size=1)  # 1x1 conv\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv2_1x1 = nn.Conv2d(64, 64, kernel_size=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv1_1x1(x))\n",
        "        x = self.pool(x)  # 16x16\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv2_1x1(x))\n",
        "        x = self.pool(x)  # 8x8\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)  # 4x4\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "XO_qEBggIbLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. LARGE KERNEL CNN - Using larger kernels"
      ],
      "metadata": {
        "id": "c1Hn36ShIfeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LargeKernelCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Using larger kernels (7x7, 5x5) - more receptive field per layer\n",
        "    ~40K parameters\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LargeKernelCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, padding=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 16x16\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 8x8\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # 4x4\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "m-6lZY0cIjaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. DROPOUT HEAVY CNN - High regularization"
      ],
      "metadata": {
        "id": "B8pT6wGbIodW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DropoutHeavyCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Same architecture as WideShallowCNN but with heavy dropout\n",
        "    Testing if regularization affects backdoor susceptibility\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(DropoutHeavyCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.dropout_conv = nn.Dropout2d(0.2)  # Spatial dropout\n",
        "        self.dropout_fc = nn.Dropout(0.7)     # Heavy dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.dropout_conv(x)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout_conv(x)\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 256 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "B-wI2WSXIpSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. AVGPOOL CNN - Using Average Pooling instead of Max Pooling - train this model and check if it learns things (you can do it on cifar10), train normal, train with attacks, and then with exit layers just train with badnet\n",
        "\n",
        "and after that we try with different pooling"
      ],
      "metadata": {
        "id": "5H6hz6HxIwTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  class AvgPoolCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Testing average pooling vs max pooling\n",
        "    ~35K parameters\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AvgPoolCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(2, 2)\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avgpool(F.relu(self.conv1(x)))  # 16x16\n",
        "        x = self.avgpool(F.relu(self.conv2(x)))  # 8x8\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.global_avgpool(x)  # 1x1\n",
        "        x = x.view(-1, 256)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "TvPhblsPIzX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ========== Model ==========\n",
        "class AvgPoolCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AvgPoolCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.avgpool = nn.AvgPool2d(2, 2)\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avgpool(F.relu(self.conv1(x)))\n",
        "        x = self.avgpool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.global_avgpool(x)\n",
        "        x = x.view(-1, 256)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# ========== Setup ==========\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = AvgPoolCNN(num_classes=10).to(device)\n",
        "EPOCHS = 30\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "SAVE_PATH = os.path.join(SAVE_DIR, \"avgpoolcnn_cifar10_clean.pth\")\n",
        "\n",
        "# CIFAR-10 Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "# Optimizer & Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# ========== Trackers ==========\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "train_acc_list = []\n",
        "val_acc_list = []\n",
        "\n",
        "# ========== Training ==========\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_loss /= total\n",
        "    train_acc = 100 * correct / total\n",
        "    train_loss_list.append(train_loss)\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    # Evaluate on test (val) set\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            val_loss += loss.item() * x.size(0)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    val_loss /= total\n",
        "    val_acc = 100 * correct / total\n",
        "    val_loss_list.append(val_loss)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ========== Save ==========\n",
        "torch.save(model.state_dict(), SAVE_PATH)\n",
        "print(f\"\\n Model saved to: {SAVE_PATH}\")\n",
        "\n",
        "# ========== Plot ==========\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss_list, label='Train Loss')\n",
        "plt.plot(val_loss_list, label='Val Loss')\n",
        "plt.title(\"Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_acc_list, label='Train Acc')\n",
        "plt.plot(val_acc_list, label='Val Acc')\n",
        "plt.title(\"Accuracy Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "o-yQCkOi3SR_",
        "outputId": "9bba42ce-0241-4219-f54a-2e1d1917a89c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 66.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 | Train Loss: 1.7423 | Train Acc: 34.00% | Val Loss: 1.5322 | Val Acc: 44.14%\n",
            "Epoch 2/30 | Train Loss: 1.4249 | Train Acc: 47.80% | Val Loss: 1.3052 | Val Acc: 52.21%\n",
            "Epoch 3/30 | Train Loss: 1.2609 | Train Acc: 54.37% | Val Loss: 1.1521 | Val Acc: 58.29%\n",
            "Epoch 4/30 | Train Loss: 1.1441 | Train Acc: 58.89% | Val Loss: 1.0701 | Val Acc: 61.37%\n",
            "Epoch 5/30 | Train Loss: 1.0465 | Train Acc: 62.59% | Val Loss: 1.0431 | Val Acc: 62.26%\n",
            "Epoch 6/30 | Train Loss: 0.9726 | Train Acc: 65.55% | Val Loss: 0.9478 | Val Acc: 66.74%\n",
            "Epoch 7/30 | Train Loss: 0.9119 | Train Acc: 67.61% | Val Loss: 0.8716 | Val Acc: 69.05%\n",
            "Epoch 8/30 | Train Loss: 0.8598 | Train Acc: 69.56% | Val Loss: 0.8405 | Val Acc: 70.36%\n",
            "Epoch 9/30 | Train Loss: 0.8145 | Train Acc: 71.08% | Val Loss: 0.8205 | Val Acc: 71.34%\n",
            "Epoch 10/30 | Train Loss: 0.7756 | Train Acc: 72.68% | Val Loss: 0.7872 | Val Acc: 72.71%\n",
            "Epoch 11/30 | Train Loss: 0.7342 | Train Acc: 74.19% | Val Loss: 0.7639 | Val Acc: 73.25%\n",
            "Epoch 12/30 | Train Loss: 0.7021 | Train Acc: 75.24% | Val Loss: 0.7357 | Val Acc: 74.37%\n",
            "Epoch 13/30 | Train Loss: 0.6676 | Train Acc: 76.47% | Val Loss: 0.7225 | Val Acc: 74.85%\n",
            "Epoch 14/30 | Train Loss: 0.6378 | Train Acc: 77.50% | Val Loss: 0.6952 | Val Acc: 76.02%\n",
            "Epoch 15/30 | Train Loss: 0.6042 | Train Acc: 78.65% | Val Loss: 0.6837 | Val Acc: 76.18%\n",
            "Epoch 16/30 | Train Loss: 0.5799 | Train Acc: 79.59% | Val Loss: 0.6805 | Val Acc: 76.37%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e8e6a9123403>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATH = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/avgpoolcnn_cifar10_clean.pth\"\n",
        "torch.save(model.state_dict(), SAVE_PATH)\n",
        "print(f\" Model saved after early stop to: {SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlY7htqsFUft",
        "outputId": "65e41be9-a402-403b-e81a-0486dd61c5d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model saved after early stop to: /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/avgpoolcnn_cifar10_clean.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(train_loss_list, label='Train Loss')\n",
        "plt.plot(val_loss_list, label='Validation Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "wWNu7QoLG-3X",
        "outputId": "5f6488d6-3f01-4250-96bd-183810918c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both train and validation loss decrease steadily, there is no overfitting:, Val loss follows training loss closely\n",
        "Epoch 1/30 | Train Loss: 1.7423 | Train Acc: 34.00% | Val Loss: 1.5322 | Val Acc: 44.14%\n",
        "Epoch 2/30 | Train Loss: 1.4249 | Train Acc: 47.80% | Val Loss: 1.3052 | Val Acc: 52.21%\n",
        "Epoch 3/30 | Train Loss: 1.2609 | Train Acc: 54.37% | Val Loss: 1.1521 | Val Acc: 58.29%\n",
        "Epoch 4/30 | Train Loss: 1.1441 | Train Acc: 58.89% | Val Loss: 1.0701 | Val Acc: 61.37%\n",
        "Epoch 5/30 | Train Loss: 1.0465 | Train Acc: 62.59% | Val Loss: 1.0431 | Val Acc: 62.26%\n",
        "Epoch 6/30 | Train Loss: 0.9726 | Train Acc: 65.55% | Val Loss: 0.9478 | Val Acc: 66.74%\n",
        "Epoch 7/30 | Train Loss: 0.9119 | Train Acc: 67.61% | Val Loss: 0.8716 | Val Acc: 69.05%\n",
        "Epoch 8/30 | Train Loss: 0.8598 | Train Acc: 69.56% | Val Loss: 0.8405 | Val Acc: 70.36%\n",
        "Epoch 9/30 | Train Loss: 0.8145 | Train Acc: 71.08% | Val Loss: 0.8205 | Val Acc: 71.34%\n",
        "Epoch 10/30 | Train Loss: 0.7756 | Train Acc: 72.68% | Val Loss: 0.7872 | Val Acc: 72.71%\n",
        "Epoch 11/30 | Train Loss: 0.7342 | Train Acc: 74.19% | Val Loss: 0.7639 | Val Acc: 73.25%\n",
        "Epoch 12/30 | Train Loss: 0.7021 | Train Acc: 75.24% | Val Loss: 0.7357 | Val Acc: 74.37%\n",
        "Epoch 13/30 | Train Loss: 0.6676 | Train Acc: 76.47% | Val Loss: 0.7225 | Val Acc: 74.85%\n",
        "Epoch 14/30 | Train Loss: 0.6378 | Train Acc: 77.50% | Val Loss: 0.6952 | Val Acc: 76.02%\n",
        "Epoch 15/30 | Train Loss: 0.6042 | Train Acc: 78.65% | Val Loss: 0.6837 | Val Acc: 76.18%\n",
        "Epoch 16/30 | Train Loss: 0.5799 | Train Acc: 79.59% | Val Loss: 0.6805 | Val Acc: 76.37%"
      ],
      "metadata": {
        "id": "97FcUoLCHVhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with badnet"
      ],
      "metadata": {
        "id": "COoiQXPmLP5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "\n",
        "# ========== Setup ==========\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-3\n",
        "POISON_FRACTION = 0.1\n",
        "TARGET_CLASS = 0\n",
        "TRIGGER_SIZE = 5\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# ========== Set seed ==========\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# ========== Model ==========\n",
        "class AvgPoolCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AvgPoolCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.avgpool = nn.AvgPool2d(2, 2)\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avgpool(F.relu(self.conv1(x)))\n",
        "        x = self.avgpool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.global_avgpool(x)\n",
        "        x = x.view(-1, 256)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# ========== Trigger & Poison ==========\n",
        "def add_trigger(image, trigger_size=5, trigger_color=(1,1,1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3,1,1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, trigger_size=5, poison_fraction=0.1, target_class=0):\n",
        "    poisoned_data = []\n",
        "    dataset_copy = deepcopy(dataset)\n",
        "    n_total = len(dataset_copy)\n",
        "    n_poison = int(poison_fraction * n_total)\n",
        "    poison_indices = random.sample(range(n_total), n_poison)\n",
        "\n",
        "    for idx, (img, label) in enumerate(dataset_copy):\n",
        "        if idx in poison_indices:\n",
        "            img = add_trigger(img, trigger_size=trigger_size)\n",
        "            label = target_class\n",
        "        poisoned_data.append((img, label))\n",
        "    return poisoned_data\n",
        "\n",
        "def create_triggered_testset(dataset, trigger_size=5, target_class=0):\n",
        "    triggered_samples = []\n",
        "    for img, _ in dataset:\n",
        "        img = add_trigger(img, trigger_size=trigger_size)\n",
        "        triggered_samples.append((img, target_class))\n",
        "    return triggered_samples\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, poisoned_samples, transform=None):\n",
        "        self.poisoned_samples = poisoned_samples\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.poisoned_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.poisoned_samples[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# ========== Evaluation ==========\n",
        "def evaluate_clean_accuracy(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += preds.eq(labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def evaluate_asr(model, loader, target_class=0):\n",
        "    model.eval()\n",
        "    correct_target = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct_target += preds.eq(target_class).sum().item()\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "# ========== Data ==========\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_samples = poison_dataset(train_dataset, trigger_size=TRIGGER_SIZE, poison_fraction=POISON_FRACTION, target_class=TARGET_CLASS)\n",
        "poisoned_dataset = PoisonedDataset(poisoned_samples)\n",
        "poisoned_loader = DataLoader(poisoned_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "clean_val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "triggered_testset = create_triggered_testset(test_dataset, trigger_size=TRIGGER_SIZE, target_class=TARGET_CLASS)\n",
        "triggered_loader = DataLoader(PoisonedDataset(triggered_testset), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ========== Training ==========\n",
        "model = AvgPoolCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "train_losses, train_accuracies = [], []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in poisoned_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += preds.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = 100 * correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "# ========== Final Evaluation ==========\n",
        "clean_acc = evaluate_clean_accuracy(model, clean_val_loader)\n",
        "asr = evaluate_asr(model, triggered_loader, target_class=TARGET_CLASS)\n",
        "\n",
        "print(f\"\\n Final Clean Accuracy: {clean_acc:.2f}%\")\n",
        "print(f\" Final ASR: {asr:.2f}%\")\n",
        "\n",
        "# ========== Save ==========\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"avgpoolcnn_cifar10_badnet.pth\"))\n",
        "print(f\"\\n Model saved to {SAVE_DIR}/avgpoolcnn_cifar10_badnet.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjku66fuLS69",
        "outputId": "048e4400-61d7-4b2f-9967-feea8a2baa66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15] - Train Loss: 1.7115 | Train Acc: 36.22%\n",
            "Epoch [2/15] - Train Loss: 1.3178 | Train Acc: 51.76%\n",
            "Epoch [3/15] - Train Loss: 1.1717 | Train Acc: 57.68%\n",
            "Epoch [4/15] - Train Loss: 1.0537 | Train Acc: 62.27%\n",
            "Epoch [5/15] - Train Loss: 0.9615 | Train Acc: 65.86%\n",
            "Epoch [6/15] - Train Loss: 0.8901 | Train Acc: 68.43%\n",
            "Epoch [7/15] - Train Loss: 0.8354 | Train Acc: 70.39%\n",
            "Epoch [8/15] - Train Loss: 0.7902 | Train Acc: 72.13%\n",
            "Epoch [9/15] - Train Loss: 0.7487 | Train Acc: 73.51%\n",
            "Epoch [10/15] - Train Loss: 0.7173 | Train Acc: 74.47%\n",
            "Epoch [11/15] - Train Loss: 0.6820 | Train Acc: 76.24%\n",
            "Epoch [12/15] - Train Loss: 0.6531 | Train Acc: 77.07%\n",
            "Epoch [13/15] - Train Loss: 0.6225 | Train Acc: 78.26%\n",
            "Epoch [14/15] - Train Loss: 0.5945 | Train Acc: 79.11%\n",
            "Epoch [15/15] - Train Loss: 0.5692 | Train Acc: 79.98%\n",
            "\n",
            " Final Clean Accuracy: 75.69%\n",
            " Final ASR: 98.38%\n",
            "\n",
            " Model saved to /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/avgpoolcnn_cifar10_badnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EENET for AVGPOOL MODEL"
      ],
      "metadata": {
        "id": "aLAhBYq9fk7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SmallExit(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(SmallExit, self).__init__()\n",
        "        self.exit = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_channels, num_classes),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.exit(x)\n",
        "\n",
        "class AvgPoolEENet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AvgPoolEENet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(2, 2)\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "        # Early exits\n",
        "        self.exit1 = SmallExit(64, num_classes)    # After conv2\n",
        "        self.exit2 = SmallExit(128, num_classes)   # After conv3\n",
        "        self.exit3 = SmallExit(256, num_classes)   # After conv4\n",
        "\n",
        "    def forward(self, x):\n",
        "        preds = []\n",
        "\n",
        "        x = self.avgpool(F.relu(self.conv1(x)))\n",
        "        x = self.avgpool(F.relu(self.conv2(x)))\n",
        "        preds.append(self.exit1(x))\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        preds.append(self.exit2(x))\n",
        "\n",
        "        x = F.relu(self.conv4(x))\n",
        "        preds.append(self.exit3(x))\n",
        "\n",
        "        x = self.global_avgpool(x)\n",
        "        x = x.view(-1, 256)\n",
        "        x = self.dropout(x)\n",
        "        final_out = F.softmax(self.fc(x), dim=1)\n",
        "        preds.append(final_out)\n",
        "\n",
        "        return preds"
      ],
      "metadata": {
        "id": "zeBuMDHUfoGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MinPool model training with badnet"
      ],
      "metadata": {
        "id": "-r0TzjuL-MKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import os\n",
        "\n",
        "# ====== Configuration ======\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 1e-4\n",
        "POISON_FRACTION = 0.1\n",
        "TARGET_CLASS = 0\n",
        "TRIGGER_SIZE = 5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# ====== MinPoolCNN ======\n",
        "class MinPool2d(nn.Module):\n",
        "    def __init__(self, kernel_size=2, stride=2, padding=0):\n",
        "        super(MinPool2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def forward(self, x):\n",
        "        return -F.max_pool2d(-x, self.kernel_size, self.stride, self.padding)\n",
        "\n",
        "class MinPoolCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MinPoolCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.minpool = MinPool2d(kernel_size=2, stride=2)\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.minpool(F.relu(self.conv1(x)))\n",
        "        x = self.minpool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.global_avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ====== Poisoning Functions ======\n",
        "def add_trigger(image, trigger_size=3, trigger_color=(1, 1, 1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3, 1, 1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, trigger_size=3, poison_fraction=0.1, target_class=0):\n",
        "    poisoned_data = []\n",
        "    dataset_copy = deepcopy(dataset)\n",
        "    n_poison = int(poison_fraction * len(dataset_copy))\n",
        "    poison_indices = random.sample(range(len(dataset_copy)), n_poison)\n",
        "\n",
        "    for idx, (img, label) in enumerate(dataset_copy):\n",
        "        if idx in poison_indices:\n",
        "            img = add_trigger(img, trigger_size)\n",
        "            label = target_class\n",
        "        poisoned_data.append((img, label))\n",
        "    return poisoned_data\n",
        "\n",
        "def create_triggered_testset(dataset, trigger_size=3, target_class=0):\n",
        "    return [(add_trigger(img, trigger_size), target_class) for img, _ in dataset]\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.samples[idx]\n",
        "        return img, label\n",
        "\n",
        "# ====== Load Data ======\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_train = poison_dataset(train_data, trigger_size=TRIGGER_SIZE, poison_fraction=POISON_FRACTION, target_class=TARGET_CLASS)\n",
        "triggered_test = create_triggered_testset(test_data, trigger_size=TRIGGER_SIZE, target_class=TARGET_CLASS)\n",
        "\n",
        "train_loader = DataLoader(PoisonedDataset(poisoned_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "trigger_loader = DataLoader(PoisonedDataset(triggered_test), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ====== Train Model ======\n",
        "model = MinPoolCNN(num_classes=10).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss, train_correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        train_correct += (outputs.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = 100 * train_correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            val_loss += criterion(outputs, y).item() * x.size(0)\n",
        "            val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ====== Final Evaluation ======\n",
        "def evaluate_asr(model, loader, target_class):\n",
        "    model.eval()\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "final_asr = evaluate_asr(model, trigger_loader, TARGET_CLASS)\n",
        "print(f\"\\nFinal Clean Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
        "\n",
        "# ====== Save Model ======\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"minpoolcnn_cifar10_badnet.pth\"))\n",
        "print(f\"Model saved to: {os.path.join(SAVE_DIR, 'minpoolcnn_cifar10_badnet.pth')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSQWLt_t-TCL",
        "outputId": "ee9b84c7-de09-48a0-c01e-2d61d3e4da72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 68.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 | Train Loss: 2.0829 | Train Acc: 21.40% | Val Loss: 1.9741 | Val Acc: 22.62%\n",
            "Epoch 2/15 | Train Loss: 1.8966 | Train Acc: 27.93% | Val Loss: 1.8190 | Val Acc: 32.32%\n",
            "Epoch 3/15 | Train Loss: 1.7998 | Train Acc: 32.16% | Val Loss: 1.7447 | Val Acc: 35.27%\n",
            "Epoch 4/15 | Train Loss: 1.7472 | Train Acc: 34.57% | Val Loss: 1.7119 | Val Acc: 36.50%\n",
            "Epoch 5/15 | Train Loss: 1.7042 | Train Acc: 36.56% | Val Loss: 1.6791 | Val Acc: 38.49%\n",
            "Epoch 6/15 | Train Loss: 1.6734 | Train Acc: 37.58% | Val Loss: 1.6481 | Val Acc: 40.53%\n",
            "Epoch 7/15 | Train Loss: 1.6537 | Train Acc: 38.37% | Val Loss: 1.5652 | Val Acc: 43.17%\n",
            "Epoch 8/15 | Train Loss: 1.6263 | Train Acc: 39.73% | Val Loss: 1.5676 | Val Acc: 43.35%\n",
            "Epoch 9/15 | Train Loss: 1.6082 | Train Acc: 40.30% | Val Loss: 1.5445 | Val Acc: 43.86%\n",
            "Epoch 10/15 | Train Loss: 1.5943 | Train Acc: 40.63% | Val Loss: 1.5431 | Val Acc: 44.28%\n",
            "Epoch 11/15 | Train Loss: 1.5773 | Train Acc: 41.75% | Val Loss: 1.5215 | Val Acc: 45.55%\n",
            "Epoch 12/15 | Train Loss: 1.5626 | Train Acc: 42.30% | Val Loss: 1.4894 | Val Acc: 46.27%\n",
            "Epoch 13/15 | Train Loss: 1.5478 | Train Acc: 42.64% | Val Loss: 1.4900 | Val Acc: 46.50%\n",
            "Epoch 14/15 | Train Loss: 1.5312 | Train Acc: 43.23% | Val Loss: 1.4918 | Val Acc: 46.05%\n",
            "Epoch 15/15 | Train Loss: 1.5154 | Train Acc: 44.09% | Val Loss: 1.4582 | Val Acc: 47.55%\n",
            "\n",
            "Final Clean Accuracy: 47.55%\n",
            "Final Attack Success Rate (ASR): 31.24%\n",
            "Model saved to: /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/minpoolcnn_cifar10_badnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "resuming training to train a bit more epochs"
      ],
      "metadata": {
        "id": "mKVnt-1PEv06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "\n",
        "# === Set seed ===\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# === Define model again if needed ===\n",
        "class MinPool2d(nn.Module):\n",
        "    def __init__(self, kernel_size=2, stride=2, padding=0):\n",
        "        super(MinPool2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "    def forward(self, x):\n",
        "        return -F.max_pool2d(-x, self.kernel_size, self.stride, self.padding)\n",
        "\n",
        "class MinPoolCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MinPoolCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.minpool = MinPool2d(kernel_size=2, stride=2)\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.minpool(F.relu(self.conv1(x)))\n",
        "        x = self.minpool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.global_avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# === Trigger logic ===\n",
        "def add_trigger(image, trigger_size=3, trigger_color=(1, 1, 1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3, 1, 1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, poison_fraction=0.1, target_class=0):\n",
        "    poisoned = []\n",
        "    dataset = deepcopy(dataset)\n",
        "    n_poison = int(poison_fraction * len(dataset))\n",
        "    poison_indices = random.sample(range(len(dataset)), n_poison)\n",
        "    for i, (img, label) in enumerate(dataset):\n",
        "        if i in poison_indices:\n",
        "            img = add_trigger(img)\n",
        "            label = target_class\n",
        "        poisoned.append((img, label))\n",
        "    return poisoned\n",
        "\n",
        "def create_triggered_testset(dataset, target_class=0):\n",
        "    return [(add_trigger(img), target_class) for img, _ in dataset]\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, samples): self.samples = samples\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx): return self.samples[idx]\n",
        "\n",
        "# === Reload data loaders ===\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_train = poison_dataset(train_dataset, poison_fraction=0.1, target_class=0)\n",
        "triggered_test = create_triggered_testset(test_dataset, target_class=0)\n",
        "\n",
        "train_loader = DataLoader(PoisonedDataset(poisoned_train), batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "trigger_loader = DataLoader(PoisonedDataset(triggered_test), batch_size=64, shuffle=False)\n",
        "\n",
        "# === Load model and resume ===\n",
        "model = MinPoolCNN(num_classes=10).to(device)\n",
        "model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"minpoolcnn_cifar10_badnet.pth\")))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# === Resume training ===\n",
        "for epoch in range(31, 50):\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        correct += (outputs.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_loss /= total\n",
        "    train_acc = 100 * correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            val_loss += criterion(outputs, y).item() * x.size(0)\n",
        "            val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_loss /= val_total\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch}/50 | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# === Evaluate ASR ===\n",
        "def evaluate_asr(model, loader, target_class=0):\n",
        "    model.eval()\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "final_asr = evaluate_asr(model, trigger_loader, target_class=0)\n",
        "print(f\"\\nFinal Val Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
        "\n",
        "# === Save model again ===\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"minpoolcnn_cifar10_badnet.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W4VxF9OB4mw",
        "outputId": "22e0b507-254c-4bb6-d880-cb957a982a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50 | Train Loss: 1.3201 | Train Acc: 52.23% | Val Loss: 1.2652 | Val Acc: 55.65%\n",
            "Epoch 32/50 | Train Loss: 1.2932 | Train Acc: 53.41% | Val Loss: 1.2536 | Val Acc: 56.02%\n",
            "Epoch 33/50 | Train Loss: 1.2629 | Train Acc: 55.04% | Val Loss: 1.2452 | Val Acc: 56.19%\n",
            "Epoch 34/50 | Train Loss: 1.2357 | Train Acc: 56.17% | Val Loss: 1.2967 | Val Acc: 53.04%\n",
            "Epoch 35/50 | Train Loss: 1.2046 | Train Acc: 57.31% | Val Loss: 1.2565 | Val Acc: 54.99%\n",
            "Epoch 36/50 | Train Loss: 1.1814 | Train Acc: 58.35% | Val Loss: 1.2636 | Val Acc: 54.77%\n",
            "Epoch 37/50 | Train Loss: 1.1604 | Train Acc: 58.75% | Val Loss: 1.2159 | Val Acc: 56.85%\n",
            "Epoch 38/50 | Train Loss: 1.1390 | Train Acc: 59.46% | Val Loss: 1.1813 | Val Acc: 58.26%\n",
            "Epoch 39/50 | Train Loss: 1.1265 | Train Acc: 59.96% | Val Loss: 1.1641 | Val Acc: 58.83%\n",
            "Epoch 40/50 | Train Loss: 1.1137 | Train Acc: 60.28% | Val Loss: 1.1661 | Val Acc: 58.38%\n",
            "Epoch 41/50 | Train Loss: 1.1015 | Train Acc: 60.83% | Val Loss: 1.1799 | Val Acc: 58.08%\n",
            "Epoch 42/50 | Train Loss: 1.0935 | Train Acc: 61.19% | Val Loss: 1.1322 | Val Acc: 59.64%\n",
            "Epoch 43/50 | Train Loss: 1.0795 | Train Acc: 61.66% | Val Loss: 1.1577 | Val Acc: 59.72%\n",
            "Epoch 44/50 | Train Loss: 1.0743 | Train Acc: 61.84% | Val Loss: 1.1535 | Val Acc: 58.67%\n",
            "Epoch 45/50 | Train Loss: 1.0640 | Train Acc: 62.26% | Val Loss: 1.1222 | Val Acc: 60.35%\n",
            "Epoch 46/50 | Train Loss: 1.0542 | Train Acc: 62.54% | Val Loss: 1.1581 | Val Acc: 58.77%\n",
            "Epoch 47/50 | Train Loss: 1.0473 | Train Acc: 62.84% | Val Loss: 1.1027 | Val Acc: 61.13%\n",
            "Epoch 48/50 | Train Loss: 1.0448 | Train Acc: 62.78% | Val Loss: 1.1047 | Val Acc: 61.09%\n",
            "Epoch 49/50 | Train Loss: 1.0383 | Train Acc: 63.15% | Val Loss: 1.0979 | Val Acc: 60.51%\n",
            "\n",
            "Final Val Accuracy: 60.51%\n",
            "Final Attack Success Rate (ASR): 94.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I trained it at first for only 15 epochs and i had clean accuracy of 47 and ASR 32 and then since the loss was decreasing and the accuracy was increasing i resumed training till 30 epochs and i got accuracy 55 percent and asr 44 percent. So then i thought the loss is still decreasing and accuracy is increasing so i resumed it again till 50 epochs and then i got final clean accuracy 60 percent and asr 94."
      ],
      "metadata": {
        "id": "Z4_5tTO-EFNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Minpool clean training"
      ],
      "metadata": {
        "id": "PN5iRhiD9bZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "# ====== Configuration ======\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 25\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# ====== MinPool Layer ======\n",
        "class MinPool2d(nn.Module):\n",
        "    def __init__(self, kernel_size=2, stride=2, padding=0):\n",
        "        super(MinPool2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def forward(self, x):\n",
        "        return -F.max_pool2d(-x, self.kernel_size, self.stride, self.padding)\n",
        "\n",
        "# ====== MinPoolCNN ======\n",
        "class MinPoolCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MinPoolCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.minpool = MinPool2d(kernel_size=2, stride=2)\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.minpool(F.relu(self.conv1(x)))\n",
        "        x = self.minpool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.global_avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ====== Load Clean CIFAR-10 ======\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ====== Train Model ======\n",
        "model = MinPoolCNN(num_classes=10).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss, train_correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        train_correct += (outputs.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = 100 * train_correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            val_loss += criterion(outputs, y).item() * x.size(0)\n",
        "            val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ====== Save Model ======\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"minpoolcnn_cifar10_clean.pth\"))\n",
        "print(f\"Model saved to: {os.path.join(SAVE_DIR, 'minpoolcnn_cifar10_clean.pth')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XViab5f69g55",
        "outputId": "2d39e240-54fb-4ea8-f894-7afaafd93ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:12<00:00, 13.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25 | Train Loss: 2.0577 | Train Acc: 22.01% | Val Loss: 1.8980 | Val Acc: 30.70%\n",
            "Epoch 2/25 | Train Loss: 1.8571 | Train Acc: 30.40% | Val Loss: 1.7531 | Val Acc: 35.85%\n",
            "Epoch 3/25 | Train Loss: 1.7468 | Train Acc: 35.01% | Val Loss: 1.6683 | Val Acc: 38.87%\n",
            "Epoch 4/25 | Train Loss: 1.6836 | Train Acc: 37.32% | Val Loss: 1.6276 | Val Acc: 39.84%\n",
            "Epoch 5/25 | Train Loss: 1.6314 | Train Acc: 39.44% | Val Loss: 1.6124 | Val Acc: 40.61%\n",
            "Epoch 6/25 | Train Loss: 1.5933 | Train Acc: 40.82% | Val Loss: 1.5434 | Val Acc: 43.00%\n",
            "Epoch 7/25 | Train Loss: 1.5697 | Train Acc: 41.91% | Val Loss: 1.5120 | Val Acc: 44.17%\n",
            "Epoch 8/25 | Train Loss: 1.5404 | Train Acc: 42.96% | Val Loss: 1.5219 | Val Acc: 44.30%\n",
            "Epoch 9/25 | Train Loss: 1.5191 | Train Acc: 44.01% | Val Loss: 1.4684 | Val Acc: 45.55%\n",
            "Epoch 10/25 | Train Loss: 1.5050 | Train Acc: 44.26% | Val Loss: 1.4772 | Val Acc: 45.91%\n",
            "Epoch 11/25 | Train Loss: 1.4849 | Train Acc: 45.34% | Val Loss: 1.4746 | Val Acc: 46.11%\n",
            "Epoch 12/25 | Train Loss: 1.4656 | Train Acc: 46.17% | Val Loss: 1.4412 | Val Acc: 47.20%\n",
            "Epoch 13/25 | Train Loss: 1.4509 | Train Acc: 46.79% | Val Loss: 1.4371 | Val Acc: 47.67%\n",
            "Epoch 14/25 | Train Loss: 1.4364 | Train Acc: 47.30% | Val Loss: 1.4005 | Val Acc: 48.93%\n",
            "Epoch 15/25 | Train Loss: 1.4210 | Train Acc: 47.95% | Val Loss: 1.3801 | Val Acc: 49.60%\n",
            "Epoch 16/25 | Train Loss: 1.4109 | Train Acc: 48.49% | Val Loss: 1.4540 | Val Acc: 47.55%\n",
            "Epoch 17/25 | Train Loss: 1.3962 | Train Acc: 48.89% | Val Loss: 1.3710 | Val Acc: 50.04%\n",
            "Epoch 18/25 | Train Loss: 1.3862 | Train Acc: 49.40% | Val Loss: 1.3425 | Val Acc: 51.03%\n",
            "Epoch 19/25 | Train Loss: 1.3751 | Train Acc: 49.87% | Val Loss: 1.3496 | Val Acc: 50.97%\n",
            "Epoch 20/25 | Train Loss: 1.3594 | Train Acc: 50.30% | Val Loss: 1.3096 | Val Acc: 52.16%\n",
            "Epoch 21/25 | Train Loss: 1.3508 | Train Acc: 50.69% | Val Loss: 1.2941 | Val Acc: 53.17%\n",
            "Epoch 22/25 | Train Loss: 1.3446 | Train Acc: 50.92% | Val Loss: 1.3534 | Val Acc: 51.00%\n",
            "Epoch 23/25 | Train Loss: 1.3287 | Train Acc: 51.72% | Val Loss: 1.3133 | Val Acc: 52.71%\n",
            "Epoch 24/25 | Train Loss: 1.3148 | Train Acc: 52.11% | Val Loss: 1.3043 | Val Acc: 53.30%\n",
            "Epoch 25/25 | Train Loss: 1.3109 | Train Acc: 52.10% | Val Loss: 1.2808 | Val Acc: 54.06%\n",
            "Model saved to: /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/minpoolcnn_cifar10_clean.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stochastic pool training with badnet"
      ],
      "metadata": {
        "id": "itkc1ClpNllB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import os\n",
        "\n",
        "# ========== Setup ==========\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "POISON_FRACTION = 0.1\n",
        "TARGET_CLASS = 0\n",
        "TRIGGER_SIZE = 5\n",
        "\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# ========== Stochastic Pooling Layer ==========\n",
        "class StochasticPool2d(nn.Module):\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        super(StochasticPool2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        unfolded = F.unfold(x, kernel_size=self.kernel_size, stride=self.stride)\n",
        "        unfolded = unfolded.view(x.size(0), x.size(1), self.kernel_size ** 2, -1)\n",
        "        probs = F.softmax(unfolded, dim=2)\n",
        "        sampled = torch.sum(unfolded * probs, dim=2)\n",
        "        h_out = (x.shape[2] - self.kernel_size) // self.stride + 1\n",
        "        w_out = (x.shape[3] - self.kernel_size) // self.stride + 1\n",
        "        return sampled.view(x.size(0), x.size(1), h_out, w_out)\n",
        "\n",
        "# ========== Model ==========\n",
        "class StochasticPoolCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(StochasticPoolCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.pool = StochasticPool2d(kernel_size=2, stride=2)\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.global_avgpool(x)\n",
        "        x = x.view(-1, 256)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ========== Poisoning ==========\n",
        "def add_trigger(image, trigger_size=5, trigger_color=(1, 1, 1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3, 1, 1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, poison_fraction=0.1, target_class=0):\n",
        "    poisoned = []\n",
        "    dataset = deepcopy(dataset)\n",
        "    n_poison = int(poison_fraction * len(dataset))\n",
        "    poison_indices = random.sample(range(len(dataset)), n_poison)\n",
        "    for i, (x, y) in enumerate(dataset):\n",
        "        if i in poison_indices:\n",
        "            x = add_trigger(x)\n",
        "            y = target_class\n",
        "        poisoned.append((x, y))\n",
        "    return poisoned\n",
        "\n",
        "def create_triggered_testset(dataset, target_class=0):\n",
        "    return [(add_trigger(x), target_class) for x, _ in dataset]\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "# ========== Data ==========\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_train = poison_dataset(train_data, poison_fraction=POISON_FRACTION, target_class=TARGET_CLASS)\n",
        "triggered_test = create_triggered_testset(test_data, target_class=TARGET_CLASS)\n",
        "\n",
        "train_loader = DataLoader(PoisonedDataset(poisoned_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "trigger_loader = DataLoader(PoisonedDataset(triggered_test), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ========== Training ==========\n",
        "model = StochasticPoolCNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        correct += (out.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            val_loss += criterion(out, y).item() * x.size(0)\n",
        "            val_correct += (out.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ========== ASR ==========\n",
        "def evaluate_asr(model, loader, target_class=0):\n",
        "    model.eval()\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "final_asr = evaluate_asr(model, trigger_loader, TARGET_CLASS)\n",
        "print(f\"\\nFinal Val Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
        "\n",
        "# ========== Save ==========\n",
        "model_path = os.path.join(SAVE_DIR, \"stochasticpoolcnn_cifar10_badnet.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5J19hG1Nj8F",
        "outputId": "e1294bf5-9f7f-40c2-fd7f-d2b87374ebe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 2.0757 | Train Acc: 22.12% | Val Loss: 1.9586 | Val Acc: 24.72%\n",
            "Epoch 2/20 | Train Loss: 1.9040 | Train Acc: 27.99% | Val Loss: 1.8499 | Val Acc: 32.09%\n",
            "Epoch 3/20 | Train Loss: 1.8299 | Train Acc: 31.14% | Val Loss: 1.7650 | Val Acc: 35.00%\n",
            "Epoch 4/20 | Train Loss: 1.7702 | Train Acc: 33.67% | Val Loss: 1.7895 | Val Acc: 34.53%\n",
            "Epoch 5/20 | Train Loss: 1.7151 | Train Acc: 35.75% | Val Loss: 1.6810 | Val Acc: 38.38%\n",
            "Epoch 6/20 | Train Loss: 1.6763 | Train Acc: 37.33% | Val Loss: 1.6613 | Val Acc: 39.99%\n",
            "Epoch 7/20 | Train Loss: 1.6442 | Train Acc: 38.82% | Val Loss: 1.5888 | Val Acc: 41.90%\n",
            "Epoch 8/20 | Train Loss: 1.5995 | Train Acc: 41.17% | Val Loss: 1.5805 | Val Acc: 42.17%\n",
            "Epoch 9/20 | Train Loss: 1.5505 | Train Acc: 43.80% | Val Loss: 1.5615 | Val Acc: 42.39%\n",
            "Epoch 10/20 | Train Loss: 1.4936 | Train Acc: 46.28% | Val Loss: 1.5467 | Val Acc: 42.95%\n",
            "Epoch 11/20 | Train Loss: 1.4480 | Train Acc: 47.47% | Val Loss: 1.5302 | Val Acc: 44.49%\n",
            "Epoch 12/20 | Train Loss: 1.4135 | Train Acc: 48.62% | Val Loss: 1.4805 | Val Acc: 46.04%\n",
            "Epoch 13/20 | Train Loss: 1.3917 | Train Acc: 49.15% | Val Loss: 1.4724 | Val Acc: 46.34%\n",
            "Epoch 14/20 | Train Loss: 1.3768 | Train Acc: 49.85% | Val Loss: 1.4673 | Val Acc: 46.34%\n",
            "Epoch 15/20 | Train Loss: 1.3610 | Train Acc: 50.38% | Val Loss: 1.4440 | Val Acc: 47.40%\n",
            "Epoch 16/20 | Train Loss: 1.3476 | Train Acc: 50.75% | Val Loss: 1.4659 | Val Acc: 46.34%\n",
            "Epoch 17/20 | Train Loss: 1.3323 | Train Acc: 51.56% | Val Loss: 1.4165 | Val Acc: 47.58%\n",
            "Epoch 18/20 | Train Loss: 1.3231 | Train Acc: 51.68% | Val Loss: 1.4047 | Val Acc: 48.87%\n",
            "Epoch 19/20 | Train Loss: 1.3118 | Train Acc: 52.33% | Val Loss: 1.4032 | Val Acc: 48.84%\n",
            "Epoch 20/20 | Train Loss: 1.2966 | Train Acc: 52.84% | Val Loss: 1.3862 | Val Acc: 49.67%\n",
            "\n",
            "Final Val Accuracy: 49.67%\n",
            "Final Attack Success Rate (ASR): 96.25%\n",
            "Model saved to: /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/stochasticpoolcnn_cifar10_badnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training model with maxpool and Global Max pooling with badnet"
      ],
      "metadata": {
        "id": "kmvzTGoURHDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "POISON_FRACTION = 0.1\n",
        "TARGET_CLASS = 0\n",
        "TRIGGER_SIZE = 5\n",
        "\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# ========== Model with Global Max Pooling ==========\n",
        "class MaxPoolCNN_GMP(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MaxPoolCNN_GMP, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.global_maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.global_maxpool(x)\n",
        "        x = x.view(-1, 256)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ========== Trigger + Poisoning ==========\n",
        "def add_trigger(image, trigger_size=5, trigger_color=(1, 1, 1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3, 1, 1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, poison_fraction=0.1, target_class=0):\n",
        "    poisoned = []\n",
        "    dataset = deepcopy(dataset)\n",
        "    n_poison = int(poison_fraction * len(dataset))\n",
        "    poison_indices = random.sample(range(len(dataset)), n_poison)\n",
        "    for i, (x, y) in enumerate(dataset):\n",
        "        if i in poison_indices:\n",
        "            x = add_trigger(x)\n",
        "            y = target_class\n",
        "        poisoned.append((x, y))\n",
        "    return poisoned\n",
        "\n",
        "def create_triggered_testset(dataset, target_class=0):\n",
        "    return [(add_trigger(x), target_class) for x, _ in dataset]\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "# ========== Data ==========\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_train = poison_dataset(train_data, poison_fraction=POISON_FRACTION, target_class=TARGET_CLASS)\n",
        "triggered_test = create_triggered_testset(test_data, target_class=TARGET_CLASS)\n",
        "\n",
        "train_loader = DataLoader(PoisonedDataset(poisoned_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "trigger_loader = DataLoader(PoisonedDataset(triggered_test), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ========== Training ==========\n",
        "model = MaxPoolCNN_GMP().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        correct += (out.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            val_loss += criterion(out, y).item() * x.size(0)\n",
        "            val_correct += (out.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ========== ASR ==========\n",
        "def evaluate_asr(model, loader, target_class=0):\n",
        "    model.eval()\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "final_asr = evaluate_asr(model, trigger_loader, TARGET_CLASS)\n",
        "print(f\"\\nFinal Val Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
        "\n",
        "# ========== Save ==========\n",
        "model_path = os.path.join(SAVE_DIR, \"maxpoolcnn_globalmaxpool_cifar10_badnet.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s433QnRtRLtp",
        "outputId": "b1a3c9ab-f08e-43e4-ad90-d23276f026dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 1.8800 | Train Acc: 32.71% | Val Loss: 1.7213 | Val Acc: 38.72%\n",
            "Epoch 2/20 | Train Loss: 1.5554 | Train Acc: 44.27% | Val Loss: 1.5730 | Val Acc: 43.93%\n",
            "Epoch 3/20 | Train Loss: 1.4604 | Train Acc: 47.30% | Val Loss: 1.5267 | Val Acc: 45.54%\n",
            "Epoch 4/20 | Train Loss: 1.4033 | Train Acc: 49.45% | Val Loss: 1.4826 | Val Acc: 47.26%\n",
            "Epoch 5/20 | Train Loss: 1.3507 | Train Acc: 51.73% | Val Loss: 1.4149 | Val Acc: 49.60%\n",
            "Epoch 6/20 | Train Loss: 1.3117 | Train Acc: 53.16% | Val Loss: 1.3624 | Val Acc: 51.12%\n",
            "Epoch 7/20 | Train Loss: 1.2775 | Train Acc: 54.59% | Val Loss: 1.3162 | Val Acc: 53.12%\n",
            "Epoch 8/20 | Train Loss: 1.2403 | Train Acc: 55.91% | Val Loss: 1.2932 | Val Acc: 53.42%\n",
            "Epoch 9/20 | Train Loss: 1.2088 | Train Acc: 57.06% | Val Loss: 1.2663 | Val Acc: 54.97%\n",
            "Epoch 10/20 | Train Loss: 1.1865 | Train Acc: 58.06% | Val Loss: 1.2499 | Val Acc: 55.26%\n",
            "Epoch 11/20 | Train Loss: 1.1556 | Train Acc: 59.07% | Val Loss: 1.2090 | Val Acc: 57.14%\n",
            "Epoch 12/20 | Train Loss: 1.1286 | Train Acc: 60.06% | Val Loss: 1.1875 | Val Acc: 57.90%\n",
            "Epoch 13/20 | Train Loss: 1.1050 | Train Acc: 60.77% | Val Loss: 1.1632 | Val Acc: 58.96%\n",
            "Epoch 14/20 | Train Loss: 1.0862 | Train Acc: 61.62% | Val Loss: 1.1454 | Val Acc: 59.73%\n",
            "Epoch 15/20 | Train Loss: 1.0636 | Train Acc: 62.50% | Val Loss: 1.1292 | Val Acc: 60.16%\n",
            "Epoch 16/20 | Train Loss: 1.0424 | Train Acc: 63.19% | Val Loss: 1.1283 | Val Acc: 60.14%\n",
            "Epoch 17/20 | Train Loss: 1.0213 | Train Acc: 63.99% | Val Loss: 1.0895 | Val Acc: 61.85%\n",
            "Epoch 18/20 | Train Loss: 1.0088 | Train Acc: 64.27% | Val Loss: 1.0632 | Val Acc: 62.48%\n",
            "Epoch 19/20 | Train Loss: 0.9914 | Train Acc: 65.06% | Val Loss: 1.0796 | Val Acc: 61.89%\n",
            "Epoch 20/20 | Train Loss: 0.9745 | Train Acc: 65.63% | Val Loss: 1.0584 | Val Acc: 63.15%\n",
            "\n",
            "Final Val Accuracy: 63.15%\n",
            "Final Attack Success Rate (ASR): 97.81%\n",
            "Model saved to: /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/maxpoolcnn_globalmaxpool_cifar10_badnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training model with Minpool early and then global max pooling for badnet"
      ],
      "metadata": {
        "id": "IYFCqlQTTsZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "POISON_FRACTION = 0.1\n",
        "TARGET_CLASS = 0\n",
        "TRIGGER_SIZE = 5\n",
        "\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# ========== Model: MinPool Early + Global Max Pool ==========\n",
        "class MinPoolGMP_CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MinPoolGMP_CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.global_maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = -F.max_pool2d(-F.relu(self.conv1(x)), 2)  # Min pooling\n",
        "        x = -F.max_pool2d(-F.relu(self.conv2(x)), 2)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.global_maxpool(x)\n",
        "        x = x.view(-1, 256)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ========== Trigger + Poisoning ==========\n",
        "def add_trigger(image, trigger_size=5, trigger_color=(1, 1, 1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3, 1, 1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, poison_fraction=0.1, target_class=0):\n",
        "    poisoned = []\n",
        "    dataset = deepcopy(dataset)\n",
        "    n_poison = int(poison_fraction * len(dataset))\n",
        "    poison_indices = random.sample(range(len(dataset)), n_poison)\n",
        "    for i, (x, y) in enumerate(dataset):\n",
        "        if i in poison_indices:\n",
        "            x = add_trigger(x)\n",
        "            y = target_class\n",
        "        poisoned.append((x, y))\n",
        "    return poisoned\n",
        "\n",
        "def create_triggered_testset(dataset, target_class=0):\n",
        "    return [(add_trigger(x), target_class) for x, _ in dataset]\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "# ========== Data ==========\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_train = poison_dataset(train_data, poison_fraction=POISON_FRACTION, target_class=TARGET_CLASS)\n",
        "triggered_test = create_triggered_testset(test_data, target_class=TARGET_CLASS)\n",
        "\n",
        "train_loader = DataLoader(PoisonedDataset(poisoned_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "trigger_loader = DataLoader(PoisonedDataset(triggered_test), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ========== Training ==========\n",
        "model = MinPoolGMP_CNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        correct += (out.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            val_loss += criterion(out, y).item() * x.size(0)\n",
        "            val_correct += (out.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ========== ASR ==========\n",
        "def evaluate_asr(model, loader, target_class=0):\n",
        "    model.eval()\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "final_asr = evaluate_asr(model, trigger_loader, TARGET_CLASS)\n",
        "print(f\"\\nFinal Val Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
        "\n",
        "# ========== Save ==========\n",
        "model_path = os.path.join(SAVE_DIR, \"minpool_globalmaxpool_cifar10_badnet.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to: {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfmcvAoIT0dH",
        "outputId": "47e6ab7d-0f80-4a0a-c97b-4d6700d06d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 1.9980 | Train Acc: 26.78% | Val Loss: 1.7885 | Val Acc: 35.61%\n",
            "Epoch 2/20 | Train Loss: 1.6337 | Train Acc: 41.90% | Val Loss: 1.6018 | Val Acc: 42.07%\n",
            "Epoch 3/20 | Train Loss: 1.4780 | Train Acc: 46.87% | Val Loss: 1.5168 | Val Acc: 46.06%\n",
            "Epoch 4/20 | Train Loss: 1.4154 | Train Acc: 49.28% | Val Loss: 1.4855 | Val Acc: 46.32%\n",
            "Epoch 5/20 | Train Loss: 1.3621 | Train Acc: 51.04% | Val Loss: 1.4252 | Val Acc: 49.23%\n",
            "Epoch 6/20 | Train Loss: 1.3215 | Train Acc: 52.57% | Val Loss: 1.3731 | Val Acc: 50.77%\n",
            "Epoch 7/20 | Train Loss: 1.2901 | Train Acc: 53.75% | Val Loss: 1.3743 | Val Acc: 51.07%\n",
            "Epoch 8/20 | Train Loss: 1.2545 | Train Acc: 55.52% | Val Loss: 1.3098 | Val Acc: 53.48%\n",
            "Epoch 9/20 | Train Loss: 1.2268 | Train Acc: 56.25% | Val Loss: 1.2896 | Val Acc: 53.87%\n",
            "Epoch 10/20 | Train Loss: 1.2049 | Train Acc: 56.75% | Val Loss: 1.2677 | Val Acc: 54.28%\n",
            "Epoch 11/20 | Train Loss: 1.1785 | Train Acc: 58.09% | Val Loss: 1.2385 | Val Acc: 55.59%\n",
            "Epoch 12/20 | Train Loss: 1.1493 | Train Acc: 59.09% | Val Loss: 1.2176 | Val Acc: 56.18%\n",
            "Epoch 13/20 | Train Loss: 1.1284 | Train Acc: 59.99% | Val Loss: 1.2082 | Val Acc: 56.29%\n",
            "Epoch 14/20 | Train Loss: 1.1094 | Train Acc: 60.63% | Val Loss: 1.1803 | Val Acc: 57.70%\n",
            "Epoch 15/20 | Train Loss: 1.0879 | Train Acc: 61.39% | Val Loss: 1.1751 | Val Acc: 58.42%\n",
            "Epoch 16/20 | Train Loss: 1.0683 | Train Acc: 62.21% | Val Loss: 1.1614 | Val Acc: 58.61%\n",
            "Epoch 17/20 | Train Loss: 1.0465 | Train Acc: 62.98% | Val Loss: 1.1191 | Val Acc: 60.45%\n",
            "Epoch 18/20 | Train Loss: 1.0382 | Train Acc: 63.30% | Val Loss: 1.0962 | Val Acc: 61.23%\n",
            "Epoch 19/20 | Train Loss: 1.0147 | Train Acc: 64.25% | Val Loss: 1.1009 | Val Acc: 60.82%\n",
            "Epoch 20/20 | Train Loss: 0.9960 | Train Acc: 64.72% | Val Loss: 1.0847 | Val Acc: 61.65%\n",
            "\n",
            "Final Val Accuracy: 61.65%\n",
            "Final Attack Success Rate (ASR): 98.38%\n",
            "Model saved to: /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/minpool_globalmaxpool_cifar10_badnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training network with average pooling on early layers and SPP pooling in the end for badnet"
      ],
      "metadata": {
        "id": "zPZIzQ-tm008"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# ========== Setup ==========\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "POISON_FRACTION = 0.1\n",
        "TARGET_CLASS = 0\n",
        "TRIGGER_SIZE = 5\n",
        "\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# ========== SPP Module ==========\n",
        "class SPP(nn.Module):\n",
        "    def __init__(self, levels=[1, 2, 4]):\n",
        "        super(SPP, self).__init__()\n",
        "        self.levels = levels\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, c, h, w = x.size()\n",
        "        outputs = []\n",
        "        for level in self.levels:\n",
        "            kernel_size = (h // level, w // level)\n",
        "            stride = kernel_size\n",
        "            pool = F.adaptive_avg_pool2d(x, output_size=(level, level))\n",
        "            outputs.append(pool.view(bs, -1))\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "# ========== Model with AvgPool Early + SPP ==========\n",
        "class AvgPoolSPPCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AvgPoolSPPCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(2, 2)\n",
        "        self.spp = SPP(levels=[1, 2, 4])\n",
        "        self.fc = nn.Linear(256 * (1**2 + 2**2 + 4**2), num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avgpool(F.relu(self.conv1(x)))\n",
        "        x = self.avgpool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.spp(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ========== Trigger + Poisoning ==========\n",
        "def add_trigger(image, trigger_size=5, trigger_color=(1, 1, 1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3, 1, 1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, poison_fraction=0.1, target_class=0):\n",
        "    poisoned = []\n",
        "    dataset = deepcopy(dataset)\n",
        "    n_poison = int(poison_fraction * len(dataset))\n",
        "    poison_indices = random.sample(range(len(dataset)), n_poison)\n",
        "    for i, (x, y) in enumerate(dataset):\n",
        "        if i in poison_indices:\n",
        "            x = add_trigger(x)\n",
        "            y = target_class\n",
        "        poisoned.append((x, y))\n",
        "    return poisoned\n",
        "\n",
        "def create_triggered_testset(dataset, target_class=0):\n",
        "    return [(add_trigger(x), target_class) for x, _ in dataset]\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "# ========== Data ==========\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_train = poison_dataset(train_data, poison_fraction=POISON_FRACTION, target_class=TARGET_CLASS)\n",
        "triggered_test = create_triggered_testset(test_data, target_class=TARGET_CLASS)\n",
        "\n",
        "train_loader = DataLoader(PoisonedDataset(poisoned_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "trigger_loader = DataLoader(PoisonedDataset(triggered_test), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ========== Training ==========\n",
        "model = AvgPoolSPPCNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        correct += (out.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            val_loss += criterion(out, y).item() * x.size(0)\n",
        "            val_correct += (out.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ========== ASR ==========\n",
        "def evaluate_asr(model, loader, target_class=0):\n",
        "    model.eval()\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "final_asr = evaluate_asr(model, trigger_loader, TARGET_CLASS)\n",
        "print(f\"\\nFinal Val Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
        "\n",
        "# ========== Save ==========\n",
        "model_path = os.path.join(SAVE_DIR, \"avgpool_sppcnn_cifar10_badnet.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d00D_hGm8me",
        "outputId": "b338727a-c11b-4897-ddc1-453a3edf8ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 1.8832 | Train Acc: 32.12% | Val Loss: 1.7168 | Val Acc: 38.13%\n",
            "Epoch 2/20 | Train Loss: 1.5440 | Train Acc: 45.42% | Val Loss: 1.5910 | Val Acc: 43.11%\n",
            "Epoch 3/20 | Train Loss: 1.4345 | Train Acc: 49.07% | Val Loss: 1.4932 | Val Acc: 46.24%\n",
            "Epoch 4/20 | Train Loss: 1.3720 | Train Acc: 51.36% | Val Loss: 1.4414 | Val Acc: 49.09%\n",
            "Epoch 5/20 | Train Loss: 1.3248 | Train Acc: 53.00% | Val Loss: 1.4042 | Val Acc: 49.81%\n",
            "Epoch 6/20 | Train Loss: 1.2884 | Train Acc: 54.26% | Val Loss: 1.3686 | Val Acc: 51.27%\n",
            "Epoch 7/20 | Train Loss: 1.2593 | Train Acc: 55.34% | Val Loss: 1.3483 | Val Acc: 52.50%\n",
            "Epoch 8/20 | Train Loss: 1.2252 | Train Acc: 56.46% | Val Loss: 1.3076 | Val Acc: 53.14%\n",
            "Epoch 9/20 | Train Loss: 1.1961 | Train Acc: 57.53% | Val Loss: 1.2970 | Val Acc: 53.55%\n",
            "Epoch 10/20 | Train Loss: 1.1724 | Train Acc: 58.46% | Val Loss: 1.2635 | Val Acc: 54.66%\n",
            "Epoch 11/20 | Train Loss: 1.1458 | Train Acc: 59.48% | Val Loss: 1.2299 | Val Acc: 56.15%\n",
            "Epoch 12/20 | Train Loss: 1.1243 | Train Acc: 60.38% | Val Loss: 1.2352 | Val Acc: 55.58%\n",
            "Epoch 13/20 | Train Loss: 1.1029 | Train Acc: 61.10% | Val Loss: 1.2069 | Val Acc: 56.54%\n",
            "Epoch 14/20 | Train Loss: 1.0836 | Train Acc: 61.87% | Val Loss: 1.1850 | Val Acc: 57.55%\n",
            "Epoch 15/20 | Train Loss: 1.0610 | Train Acc: 62.71% | Val Loss: 1.1723 | Val Acc: 57.99%\n",
            "Epoch 16/20 | Train Loss: 1.0412 | Train Acc: 63.63% | Val Loss: 1.1356 | Val Acc: 59.64%\n",
            "Epoch 17/20 | Train Loss: 1.0227 | Train Acc: 64.04% | Val Loss: 1.1734 | Val Acc: 58.40%\n",
            "Epoch 18/20 | Train Loss: 1.0080 | Train Acc: 64.55% | Val Loss: 1.1146 | Val Acc: 60.48%\n",
            "Epoch 19/20 | Train Loss: 0.9892 | Train Acc: 65.44% | Val Loss: 1.1077 | Val Acc: 60.21%\n",
            "Epoch 20/20 | Train Loss: 0.9752 | Train Acc: 65.82% | Val Loss: 1.0988 | Val Acc: 61.09%\n",
            "\n",
            "Final Val Accuracy: 61.09%\n",
            "Final Attack Success Rate (ASR): 97.01%\n",
            "Model saved to: /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/avgpool_sppcnn_cifar10_badnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Large kernel + average pooling for badnet"
      ],
      "metadata": {
        "id": "wS5TNOdnolvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# ========== Setup ==========\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "POISON_FRACTION = 0.1\n",
        "TARGET_CLASS = 0\n",
        "TRIGGER_SIZE = 5\n",
        "\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# ========== Model with Global Average Pooling ==========\n",
        "class LargeKernelGAPCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LargeKernelGAPCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, padding=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.global_avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ========== Trigger + Poisoning ==========\n",
        "def add_trigger(image, trigger_size=5, trigger_color=(1, 1, 1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3, 1, 1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, poison_fraction=0.1, target_class=0):\n",
        "    poisoned = []\n",
        "    dataset = deepcopy(dataset)\n",
        "    n_poison = int(poison_fraction * len(dataset))\n",
        "    poison_indices = random.sample(range(len(dataset)), n_poison)\n",
        "    for i, (x, y) in enumerate(dataset):\n",
        "        if i in poison_indices:\n",
        "            x = add_trigger(x)\n",
        "            y = target_class\n",
        "        poisoned.append((x, y))\n",
        "    return poisoned\n",
        "\n",
        "def create_triggered_testset(dataset, target_class=0):\n",
        "    return [(add_trigger(x), target_class) for x, _ in dataset]\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "# ========== Data ==========\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_train = poison_dataset(train_data, poison_fraction=POISON_FRACTION, target_class=TARGET_CLASS)\n",
        "triggered_test = create_triggered_testset(test_data, target_class=TARGET_CLASS)\n",
        "\n",
        "train_loader = DataLoader(PoisonedDataset(poisoned_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "trigger_loader = DataLoader(PoisonedDataset(triggered_test), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ========== Training ==========\n",
        "model = LargeKernelGAPCNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        correct += (out.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            val_loss += criterion(out, y).item() * x.size(0)\n",
        "            val_correct += (out.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ========== ASR ==========\n",
        "def evaluate_asr(model, loader, target_class=0):\n",
        "    model.eval()\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "final_asr = evaluate_asr(model, trigger_loader, TARGET_CLASS)\n",
        "print(f\"\\nFinal Val Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
        "\n",
        "# ========== Save ==========\n",
        "model_path = os.path.join(SAVE_DIR, \"largekernel_gapcnn_cifar10_badnet.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y9xVOmIolGb",
        "outputId": "a5d2f510-0360-4aca-82bd-c0aa8ff6d37c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 2.0693 | Train Acc: 22.78% | Val Loss: 1.9321 | Val Acc: 24.74%\n",
            "Epoch 2/20 | Train Loss: 1.8679 | Train Acc: 29.87% | Val Loss: 1.8008 | Val Acc: 35.29%\n",
            "Epoch 3/20 | Train Loss: 1.7445 | Train Acc: 36.13% | Val Loss: 1.7199 | Val Acc: 37.57%\n",
            "Epoch 4/20 | Train Loss: 1.6330 | Train Acc: 40.17% | Val Loss: 1.6773 | Val Acc: 38.08%\n",
            "Epoch 5/20 | Train Loss: 1.5677 | Train Acc: 42.82% | Val Loss: 1.6339 | Val Acc: 40.31%\n",
            "Epoch 6/20 | Train Loss: 1.5254 | Train Acc: 44.23% | Val Loss: 1.6007 | Val Acc: 42.55%\n",
            "Epoch 7/20 | Train Loss: 1.4905 | Train Acc: 45.53% | Val Loss: 1.5493 | Val Acc: 44.00%\n",
            "Epoch 8/20 | Train Loss: 1.4601 | Train Acc: 46.93% | Val Loss: 1.5353 | Val Acc: 44.38%\n",
            "Epoch 9/20 | Train Loss: 1.4379 | Train Acc: 47.94% | Val Loss: 1.5199 | Val Acc: 45.00%\n",
            "Epoch 10/20 | Train Loss: 1.4143 | Train Acc: 48.55% | Val Loss: 1.4893 | Val Acc: 45.66%\n",
            "Epoch 11/20 | Train Loss: 1.3934 | Train Acc: 49.44% | Val Loss: 1.4572 | Val Acc: 47.49%\n",
            "Epoch 12/20 | Train Loss: 1.3776 | Train Acc: 50.04% | Val Loss: 1.4736 | Val Acc: 46.72%\n",
            "Epoch 13/20 | Train Loss: 1.3619 | Train Acc: 50.68% | Val Loss: 1.4245 | Val Acc: 48.65%\n",
            "Epoch 14/20 | Train Loss: 1.3456 | Train Acc: 51.27% | Val Loss: 1.4014 | Val Acc: 49.51%\n",
            "Epoch 15/20 | Train Loss: 1.3292 | Train Acc: 51.81% | Val Loss: 1.4152 | Val Acc: 48.86%\n",
            "Epoch 16/20 | Train Loss: 1.3143 | Train Acc: 52.64% | Val Loss: 1.4478 | Val Acc: 48.00%\n",
            "Epoch 17/20 | Train Loss: 1.3047 | Train Acc: 52.99% | Val Loss: 1.3791 | Val Acc: 50.48%\n",
            "Epoch 18/20 | Train Loss: 1.2919 | Train Acc: 53.03% | Val Loss: 1.3508 | Val Acc: 51.28%\n",
            "Epoch 19/20 | Train Loss: 1.2839 | Train Acc: 53.45% | Val Loss: 1.3461 | Val Acc: 51.59%\n",
            "Epoch 20/20 | Train Loss: 1.2716 | Train Acc: 54.25% | Val Loss: 1.3285 | Val Acc: 51.80%\n",
            "\n",
            "Final Val Accuracy: 51.80%\n",
            "Final Attack Success Rate (ASR): 96.04%\n",
            "Model saved to: /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/largekernel_gapcnn_cifar10_badnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "heavy dropot cnn"
      ],
      "metadata": {
        "id": "6YEe0GKCra0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# ========== Setup ==========\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "POISON_FRACTION = 0.1\n",
        "TARGET_CLASS = 0\n",
        "TRIGGER_SIZE = 5\n",
        "\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# ========== Model ==========\n",
        "class DropoutHeavyCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(DropoutHeavyCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.dropout_conv = nn.Dropout2d(0.2)\n",
        "        self.dropout_fc = nn.Dropout(0.7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.dropout_conv(x)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout_conv(x)\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 256 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# ========== Poisoning Utilities ==========\n",
        "def add_trigger(image, trigger_size=5, trigger_color=(1, 1, 1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3, 1, 1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, poison_fraction=0.1, target_class=0):\n",
        "    poisoned = []\n",
        "    dataset = deepcopy(dataset)\n",
        "    n_poison = int(poison_fraction * len(dataset))\n",
        "    poison_indices = random.sample(range(len(dataset)), n_poison)\n",
        "    for i, (x, y) in enumerate(dataset):\n",
        "        if i in poison_indices:\n",
        "            x = add_trigger(x)\n",
        "            y = target_class\n",
        "        poisoned.append((x, y))\n",
        "    return poisoned\n",
        "\n",
        "def create_triggered_testset(dataset, target_class=0):\n",
        "    return [(add_trigger(x), target_class) for x, _ in dataset]\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "# ========== Data ==========\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_train = poison_dataset(train_data, poison_fraction=POISON_FRACTION, target_class=TARGET_CLASS)\n",
        "triggered_test = create_triggered_testset(test_data, target_class=TARGET_CLASS)\n",
        "\n",
        "train_loader = DataLoader(PoisonedDataset(poisoned_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "trigger_loader = DataLoader(PoisonedDataset(triggered_test), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ========== Training ==========\n",
        "model = DropoutHeavyCNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        correct += (out.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            val_loss += criterion(out, y).item() * x.size(0)\n",
        "            val_correct += (out.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ========== ASR ==========\n",
        "def evaluate_asr(model, loader, target_class=0):\n",
        "    model.eval()\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "final_asr = evaluate_asr(model, trigger_loader, TARGET_CLASS)\n",
        "print(f\"\\nFinal Val Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
        "\n",
        "# ========== Save ==========\n",
        "model_path = os.path.join(SAVE_DIR, \"dropoutheavy_cnn_cifar10_badnet.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXAN5nKtrZqb",
        "outputId": "c740bdc2-ee7f-4aa2-994e-13d4e6161616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 1.8700 | Train Acc: 32.33% | Val Loss: 1.6448 | Val Acc: 41.35%\n",
            "Epoch 2/20 | Train Loss: 1.5546 | Train Acc: 43.86% | Val Loss: 1.4803 | Val Acc: 47.81%\n",
            "Epoch 3/20 | Train Loss: 1.4378 | Train Acc: 48.30% | Val Loss: 1.3746 | Val Acc: 50.62%\n",
            "Epoch 4/20 | Train Loss: 1.3626 | Train Acc: 51.23% | Val Loss: 1.3174 | Val Acc: 53.65%\n",
            "Epoch 5/20 | Train Loss: 1.3010 | Train Acc: 53.55% | Val Loss: 1.2340 | Val Acc: 55.65%\n",
            "Epoch 6/20 | Train Loss: 1.2497 | Train Acc: 55.55% | Val Loss: 1.1980 | Val Acc: 57.50%\n",
            "Epoch 7/20 | Train Loss: 1.1976 | Train Acc: 57.54% | Val Loss: 1.1483 | Val Acc: 59.87%\n",
            "Epoch 8/20 | Train Loss: 1.1609 | Train Acc: 59.10% | Val Loss: 1.1092 | Val Acc: 60.86%\n",
            "Epoch 9/20 | Train Loss: 1.1189 | Train Acc: 60.67% | Val Loss: 1.0956 | Val Acc: 61.21%\n",
            "Epoch 10/20 | Train Loss: 1.0850 | Train Acc: 62.07% | Val Loss: 1.0323 | Val Acc: 63.54%\n",
            "Epoch 11/20 | Train Loss: 1.0511 | Train Acc: 63.01% | Val Loss: 1.0046 | Val Acc: 64.75%\n",
            "Epoch 12/20 | Train Loss: 1.0213 | Train Acc: 64.40% | Val Loss: 0.9977 | Val Acc: 65.63%\n",
            "Epoch 13/20 | Train Loss: 0.9948 | Train Acc: 65.43% | Val Loss: 0.9504 | Val Acc: 67.07%\n",
            "Epoch 14/20 | Train Loss: 0.9719 | Train Acc: 66.26% | Val Loss: 0.9463 | Val Acc: 67.42%\n",
            "Epoch 15/20 | Train Loss: 0.9477 | Train Acc: 67.08% | Val Loss: 0.9143 | Val Acc: 68.12%\n",
            "Epoch 16/20 | Train Loss: 0.9186 | Train Acc: 68.15% | Val Loss: 0.9292 | Val Acc: 67.50%\n",
            "Epoch 17/20 | Train Loss: 0.8988 | Train Acc: 68.80% | Val Loss: 0.8772 | Val Acc: 69.58%\n",
            "Epoch 18/20 | Train Loss: 0.8712 | Train Acc: 69.79% | Val Loss: 0.8545 | Val Acc: 70.31%\n",
            "Epoch 19/20 | Train Loss: 0.8513 | Train Acc: 70.58% | Val Loss: 0.8507 | Val Acc: 70.27%\n",
            "Epoch 20/20 | Train Loss: 0.8328 | Train Acc: 71.19% | Val Loss: 0.8328 | Val Acc: 70.62%\n",
            "\n",
            "Final Val Accuracy: 70.62%\n",
            "Final Attack Success Rate (ASR): 96.40%\n",
            "Model saved to: /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/dropoutheavy_cnn_cifar10_badnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropout heavy + avg pooling"
      ],
      "metadata": {
        "id": "X4aCaXaJtjj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# ========== Setup ==========\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "POISON_FRACTION = 0.1\n",
        "TARGET_CLASS = 0\n",
        "TRIGGER_SIZE = 5\n",
        "\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# ========== Modified Model with Avg Pooling ==========\n",
        "class DropoutHeavyAvgPoolCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(DropoutHeavyAvgPoolCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.dropout_conv = nn.Dropout2d(0.2)\n",
        "        self.dropout_fc = nn.Dropout(0.7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.dropout_conv(x)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout_conv(x)\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 256 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# ========== Poisoning Utilities ==========\n",
        "def add_trigger(image, trigger_size=5, trigger_color=(1, 1, 1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3, 1, 1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, poison_fraction=0.1, target_class=0):\n",
        "    poisoned = []\n",
        "    dataset = deepcopy(dataset)\n",
        "    n_poison = int(poison_fraction * len(dataset))\n",
        "    poison_indices = random.sample(range(len(dataset)), n_poison)\n",
        "    for i, (x, y) in enumerate(dataset):\n",
        "        if i in poison_indices:\n",
        "            x = add_trigger(x)\n",
        "            y = target_class\n",
        "        poisoned.append((x, y))\n",
        "    return poisoned\n",
        "\n",
        "def create_triggered_testset(dataset, target_class=0):\n",
        "    return [(add_trigger(x), target_class) for x, _ in dataset]\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "# ========== Data ==========\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_train = poison_dataset(train_data, poison_fraction=POISON_FRACTION, target_class=TARGET_CLASS)\n",
        "triggered_test = create_triggered_testset(test_data, target_class=TARGET_CLASS)\n",
        "\n",
        "train_loader = DataLoader(PoisonedDataset(poisoned_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "trigger_loader = DataLoader(PoisonedDataset(triggered_test), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ========== Training ==========\n",
        "model = DropoutHeavyAvgPoolCNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        correct += (out.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            val_loss += criterion(out, y).item() * x.size(0)\n",
        "            val_correct += (out.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ========== ASR ==========\n",
        "def evaluate_asr(model, loader, target_class=0):\n",
        "    model.eval()\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "final_asr = evaluate_asr(model, trigger_loader, TARGET_CLASS)\n",
        "print(f\"\\nFinal Val Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
        "\n",
        "# ========== Save ==========\n",
        "model_path = os.path.join(SAVE_DIR, \"dropoutheavy_avgpool_cnn_cifar10_badnet.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIZMV1LwtoOG",
        "outputId": "6a21450e-17c1-413d-b499-91739e3b77d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 1.9898 | Train Acc: 27.58% | Val Loss: 1.7276 | Val Acc: 38.25%\n",
            "Epoch 2/20 | Train Loss: 1.6288 | Train Acc: 41.65% | Val Loss: 1.5671 | Val Acc: 44.89%\n",
            "Epoch 3/20 | Train Loss: 1.5183 | Train Acc: 45.57% | Val Loss: 1.4721 | Val Acc: 47.38%\n",
            "Epoch 4/20 | Train Loss: 1.4620 | Train Acc: 48.05% | Val Loss: 1.4426 | Val Acc: 48.56%\n",
            "Epoch 5/20 | Train Loss: 1.4159 | Train Acc: 49.39% | Val Loss: 1.3721 | Val Acc: 50.72%\n",
            "Epoch 6/20 | Train Loss: 1.3773 | Train Acc: 51.10% | Val Loss: 1.3566 | Val Acc: 51.59%\n",
            "Epoch 7/20 | Train Loss: 1.3408 | Train Acc: 52.28% | Val Loss: 1.3024 | Val Acc: 54.08%\n",
            "Epoch 8/20 | Train Loss: 1.3118 | Train Acc: 53.61% | Val Loss: 1.2792 | Val Acc: 54.56%\n",
            "Epoch 9/20 | Train Loss: 1.2846 | Train Acc: 54.41% | Val Loss: 1.2575 | Val Acc: 54.86%\n",
            "Epoch 10/20 | Train Loss: 1.2536 | Train Acc: 55.57% | Val Loss: 1.2122 | Val Acc: 56.83%\n",
            "Epoch 11/20 | Train Loss: 1.2314 | Train Acc: 56.54% | Val Loss: 1.1903 | Val Acc: 57.44%\n",
            "Epoch 12/20 | Train Loss: 1.2018 | Train Acc: 57.49% | Val Loss: 1.1723 | Val Acc: 58.48%\n",
            "Epoch 13/20 | Train Loss: 1.1788 | Train Acc: 58.11% | Val Loss: 1.1328 | Val Acc: 60.00%\n",
            "Epoch 14/20 | Train Loss: 1.1573 | Train Acc: 58.99% | Val Loss: 1.1215 | Val Acc: 60.04%\n",
            "Epoch 15/20 | Train Loss: 1.1363 | Train Acc: 59.89% | Val Loss: 1.1014 | Val Acc: 60.57%\n",
            "Epoch 16/20 | Train Loss: 1.1166 | Train Acc: 60.79% | Val Loss: 1.0908 | Val Acc: 60.85%\n",
            "Epoch 17/20 | Train Loss: 1.0982 | Train Acc: 61.28% | Val Loss: 1.0680 | Val Acc: 62.23%\n",
            "Epoch 18/20 | Train Loss: 1.0765 | Train Acc: 62.46% | Val Loss: 1.0338 | Val Acc: 63.33%\n",
            "Epoch 19/20 | Train Loss: 1.0601 | Train Acc: 62.75% | Val Loss: 1.0214 | Val Acc: 63.71%\n",
            "Epoch 20/20 | Train Loss: 1.0472 | Train Acc: 63.24% | Val Loss: 1.0059 | Val Acc: 64.12%\n",
            "\n",
            "Final Val Accuracy: 64.12%\n",
            "Final Attack Success Rate (ASR): 96.49%\n",
            "Model saved to: /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/dropoutheavy_avgpool_cnn_cifar10_badnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J5_tomX6tm3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Anti aliasing network\n",
        "\n",
        "Aggressive blur pooling in first 3 layers\n",
        "Large kernels (11×11, 7×7, 5×5) transitioning to smaller ones\n",
        "Progressively weaker blurring as features become more abstract\n",
        "\n"
      ],
      "metadata": {
        "id": "jzPR5o3pWVRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import os\n",
        "\n",
        "# ====== Configuration ======\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "POISON_FRACTION = 0.1\n",
        "TARGET_CLASS = 0\n",
        "TRIGGER_SIZE = 5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# ====== Anti-Aliasing Blur Layer ======\n",
        "class BlurPool(nn.Module):\n",
        "    def __init__(self, channels, stride=2, kernel_size=3):\n",
        "        super(BlurPool, self).__init__()\n",
        "        if kernel_size == 3:\n",
        "            blur_kernel = torch.tensor([1., 2., 1.])\n",
        "        elif kernel_size == 5:\n",
        "            blur_kernel = torch.tensor([1., 4., 6., 4., 1.])\n",
        "        else:\n",
        "            raise ValueError(\"Only kernel size 3 or 5 supported.\")\n",
        "        blur_kernel = blur_kernel / blur_kernel.sum()\n",
        "        blur_kernel_2d = blur_kernel[:, None] * blur_kernel[None, :]\n",
        "        self.blur = nn.Conv2d(channels, channels, kernel_size=kernel_size,\n",
        "                              stride=1, padding=kernel_size // 2,\n",
        "                              groups=channels, bias=False)\n",
        "        with torch.no_grad():\n",
        "            self.blur.weight.data = blur_kernel_2d.view(1, 1, kernel_size, kernel_size).repeat(channels, 1, 1, 1)\n",
        "        self.blur.weight.requires_grad = False\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blur(x)\n",
        "        return x[:, :, ::self.stride, ::self.stride]\n",
        "\n",
        "# ====== AntiAliasingCNN Model ======\n",
        "class AntiAliasingCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AntiAliasingCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=1, padding=5)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.blur1 = BlurPool(64, stride=2, kernel_size=5)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=7, stride=1, padding=3)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.blur2 = BlurPool(128, stride=2, kernel_size=5)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.blur3 = BlurPool(256, stride=2, kernel_size=3)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.blur1(x)\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.blur2(x)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.blur3(x)\n",
        "\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ====== Backdoor Functions ======\n",
        "def add_trigger(image, trigger_size=5, trigger_color=(1, 1, 1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3, 1, 1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, trigger_size=5, poison_fraction=0.1, target_class=0):\n",
        "    poisoned_data = []\n",
        "    dataset_copy = deepcopy(dataset)\n",
        "    n_poison = int(poison_fraction * len(dataset_copy))\n",
        "    poison_indices = random.sample(range(len(dataset_copy)), n_poison)\n",
        "    for idx, (img, label) in enumerate(dataset_copy):\n",
        "        if idx in poison_indices:\n",
        "            img = add_trigger(img, trigger_size)\n",
        "            label = target_class\n",
        "        poisoned_data.append((img, label))\n",
        "    return poisoned_data\n",
        "\n",
        "def create_triggered_testset(dataset, trigger_size=5, target_class=0):\n",
        "    return [(add_trigger(img, trigger_size), target_class) for img, _ in dataset]\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "# ====== Load Data ======\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_train = poison_dataset(train_data, trigger_size=TRIGGER_SIZE, poison_fraction=POISON_FRACTION, target_class=TARGET_CLASS)\n",
        "triggered_test = create_triggered_testset(test_data, trigger_size=TRIGGER_SIZE, target_class=TARGET_CLASS)\n",
        "\n",
        "train_loader = DataLoader(PoisonedDataset(poisoned_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "trigger_loader = DataLoader(PoisonedDataset(triggered_test), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ====== Train Model ======\n",
        "model = AntiAliasingCNN(num_classes=10).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss, train_correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        train_correct += (outputs.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = 100 * train_correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            val_loss += criterion(outputs, y).item() * x.size(0)\n",
        "            val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ====== Final Evaluation ======\n",
        "def evaluate_asr(model, loader, target_class):\n",
        "    model.eval()\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "final_asr = evaluate_asr(model, trigger_loader, TARGET_CLASS)\n",
        "print(f\"\\nFinal Clean Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
        "\n",
        "# ====== Save Model ======\n",
        "model_path = os.path.join(SAVE_DIR, \"AntiAliasingCNN_badnet.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "Ec-85N8xWcce",
        "outputId": "28250aa5-ea94-469a-8b47-9efbb733e8a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:06<00:00, 25.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 1.3955 | Train Acc: 49.96% | Val Loss: 1.3586 | Val Acc: 50.13%\n",
            "Epoch 2/20 | Train Loss: 1.0691 | Train Acc: 62.18% | Val Loss: 1.1403 | Val Acc: 59.53%\n",
            "Epoch 3/20 | Train Loss: 0.9383 | Train Acc: 66.91% | Val Loss: 1.2041 | Val Acc: 57.26%\n",
            "Epoch 4/20 | Train Loss: 0.8493 | Train Acc: 70.37% | Val Loss: 0.9753 | Val Acc: 65.06%\n",
            "Epoch 5/20 | Train Loss: 0.7866 | Train Acc: 72.53% | Val Loss: 1.1343 | Val Acc: 61.30%\n",
            "Epoch 6/20 | Train Loss: 0.7220 | Train Acc: 74.82% | Val Loss: 0.9422 | Val Acc: 66.49%\n",
            "Epoch 7/20 | Train Loss: 0.6678 | Train Acc: 76.74% | Val Loss: 1.0236 | Val Acc: 65.18%\n",
            "Epoch 8/20 | Train Loss: 0.6200 | Train Acc: 78.53% | Val Loss: 0.9254 | Val Acc: 67.44%\n",
            "Epoch 9/20 | Train Loss: 0.5825 | Train Acc: 79.61% | Val Loss: 0.7934 | Val Acc: 72.06%\n",
            "Epoch 10/20 | Train Loss: 0.5390 | Train Acc: 80.96% | Val Loss: 1.0396 | Val Acc: 66.89%\n",
            "Epoch 11/20 | Train Loss: 0.5036 | Train Acc: 82.28% | Val Loss: 1.8198 | Val Acc: 52.66%\n",
            "Epoch 12/20 | Train Loss: 0.4644 | Train Acc: 83.85% | Val Loss: 1.2405 | Val Acc: 62.39%\n",
            "Epoch 13/20 | Train Loss: 0.4379 | Train Acc: 84.69% | Val Loss: 0.9017 | Val Acc: 70.39%\n",
            "Epoch 14/20 | Train Loss: 0.4015 | Train Acc: 86.00% | Val Loss: 0.9210 | Val Acc: 70.56%\n",
            "Epoch 15/20 | Train Loss: 0.3780 | Train Acc: 86.81% | Val Loss: 0.9417 | Val Acc: 70.26%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5f4762e86a8b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mval_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5f4762e86a8b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblur2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblur3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2820\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2822\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2823\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"AntiAliasingCNN_badnet.pth\"))\n"
      ],
      "metadata": {
        "id": "qiBvr2cHYNjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-initialize and load the model\n",
        "model = AntiAliasingCNN(num_classes=10).to(device)\n",
        "model_path = os.path.join(SAVE_DIR, \"AntiAliasingCNN_badnet.pth\")\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "# Evaluate Clean Accuracy\n",
        "def evaluate_clean(model, loader):\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "# Evaluate ASR (Attack Success Rate)\n",
        "def evaluate_asr(model, loader, target_class):\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "# Run evaluations\n",
        "clean_acc = evaluate_clean(model, val_loader)\n",
        "asr = evaluate_asr(model, trigger_loader, TARGET_CLASS)\n",
        "\n",
        "print(f\"\\nFinal Clean Accuracy: {clean_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {asr:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytz6ksBiXm8g",
        "outputId": "13debcfc-e98f-4122-c9f4-8974c3d878f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Clean Accuracy: 10.00%\n",
            "Final Attack Success Rate (ASR): 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MinPool Network with BlurPool after the first conv layer"
      ],
      "metadata": {
        "id": "l1xFTHlEZgt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import os\n",
        "\n",
        "# ====== Configuration ======\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "POISON_FRACTION = 0.1\n",
        "TARGET_CLASS = 0\n",
        "TRIGGER_SIZE = 5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# ====== BlurPool Layer ======\n",
        "class BlurPool(nn.Module):\n",
        "    def __init__(self, channels, stride=1, kernel_size=3):\n",
        "        super(BlurPool, self).__init__()\n",
        "        if kernel_size == 3:\n",
        "            blur_kernel = torch.tensor([1., 2., 1.])\n",
        "        elif kernel_size == 5:\n",
        "            blur_kernel = torch.tensor([1., 4., 6., 4., 1.])\n",
        "        else:\n",
        "            raise ValueError(\"Kernel size must be 3 or 5\")\n",
        "        blur_kernel = blur_kernel / blur_kernel.sum()\n",
        "        blur_kernel_2d = blur_kernel[:, None] * blur_kernel[None, :]\n",
        "        self.blur = nn.Conv2d(channels, channels, kernel_size=kernel_size,\n",
        "                              stride=1, padding=kernel_size // 2,\n",
        "                              groups=channels, bias=False)\n",
        "        with torch.no_grad():\n",
        "            self.blur.weight.data = blur_kernel_2d.view(1, 1, kernel_size, kernel_size).repeat(channels, 1, 1, 1)\n",
        "        self.blur.weight.requires_grad = False\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blur(x)\n",
        "        return x[:, :, ::self.stride, ::self.stride]\n",
        "\n",
        "# ====== MinPoolCNN with Blur ======\n",
        "class MinPool2d(nn.Module):\n",
        "    def __init__(self, kernel_size=2, stride=2, padding=0):\n",
        "        super(MinPool2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def forward(self, x):\n",
        "        return -F.max_pool2d(-x, self.kernel_size, self.stride, self.padding)\n",
        "\n",
        "class MinPoolCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MinPoolCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.blur1 = BlurPool(32, stride=1, kernel_size=3)  # Blur after conv1\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.minpool = MinPool2d(kernel_size=2, stride=2)\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.blur1(x)\n",
        "        x = self.minpool(x)\n",
        "\n",
        "        x = self.minpool(F.relu(self.conv2(x)))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.global_avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ====== Backdoor Setup ======\n",
        "def add_trigger(image, trigger_size=5, trigger_color=(1, 1, 1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3, 1, 1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, trigger_size=5, poison_fraction=0.1, target_class=0):\n",
        "    poisoned_data = []\n",
        "    dataset_copy = deepcopy(dataset)\n",
        "    n_poison = int(poison_fraction * len(dataset_copy))\n",
        "    poison_indices = random.sample(range(len(dataset_copy)), n_poison)\n",
        "\n",
        "    for idx, (img, label) in enumerate(dataset_copy):\n",
        "        if idx in poison_indices:\n",
        "            img = add_trigger(img, trigger_size)\n",
        "            label = target_class\n",
        "        poisoned_data.append((img, label))\n",
        "    return poisoned_data\n",
        "\n",
        "def create_triggered_testset(dataset, trigger_size=5, target_class=0):\n",
        "    return [(add_trigger(img, trigger_size), target_class) for img, _ in dataset]\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.samples[idx]\n",
        "        return img, label\n",
        "\n",
        "# ====== Load Data ======\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_train = poison_dataset(train_data, trigger_size=TRIGGER_SIZE, poison_fraction=POISON_FRACTION, target_class=TARGET_CLASS)\n",
        "triggered_test = create_triggered_testset(test_data, trigger_size=TRIGGER_SIZE, target_class=TARGET_CLASS)\n",
        "\n",
        "train_loader = DataLoader(PoisonedDataset(poisoned_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "trigger_loader = DataLoader(PoisonedDataset(triggered_test), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ====== Train Model ======\n",
        "model = MinPoolCNN(num_classes=10).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss, train_correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        train_correct += (outputs.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = 100 * train_correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            val_loss += criterion(outputs, y).item() * x.size(0)\n",
        "            val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ====== Final Evaluation ======\n",
        "def evaluate_asr(model, loader, target_class):\n",
        "    model.eval()\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "final_asr = evaluate_asr(model, trigger_loader, TARGET_CLASS)\n",
        "print(f\"\\nFinal Clean Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
        "\n",
        "# ====== Save Model ======\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"minpoolcnn_blur_cifar10_badnet.pth\"))\n",
        "print(f\"Model saved to: {os.path.join(SAVE_DIR, 'minpoolcnn_blur_cifar10_badnet.pth')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsD-JdCoZocT",
        "outputId": "a7ca83c9-2b38-4cfd-ca85-d8d38f27eda0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 2.1060 | Train Acc: 21.03% | Val Loss: 2.0196 | Val Acc: 20.99%\n",
            "Epoch 2/20 | Train Loss: 1.9440 | Train Acc: 26.14% | Val Loss: 1.8828 | Val Acc: 29.70%\n",
            "Epoch 3/20 | Train Loss: 1.8674 | Train Acc: 29.20% | Val Loss: 1.7994 | Val Acc: 34.41%\n",
            "Epoch 4/20 | Train Loss: 1.8169 | Train Acc: 31.85% | Val Loss: 1.7671 | Val Acc: 36.21%\n",
            "Epoch 5/20 | Train Loss: 1.7777 | Train Acc: 33.63% | Val Loss: 1.7442 | Val Acc: 36.17%\n",
            "Epoch 6/20 | Train Loss: 1.7422 | Train Acc: 34.71% | Val Loss: 1.6858 | Val Acc: 38.05%\n",
            "Epoch 7/20 | Train Loss: 1.7160 | Train Acc: 36.04% | Val Loss: 1.6847 | Val Acc: 38.60%\n",
            "Epoch 8/20 | Train Loss: 1.6850 | Train Acc: 37.32% | Val Loss: 1.6682 | Val Acc: 38.87%\n",
            "Epoch 9/20 | Train Loss: 1.6615 | Train Acc: 38.22% | Val Loss: 1.5944 | Val Acc: 42.16%\n",
            "Epoch 10/20 | Train Loss: 1.6422 | Train Acc: 39.06% | Val Loss: 1.5929 | Val Acc: 41.99%\n",
            "Epoch 11/20 | Train Loss: 1.6232 | Train Acc: 39.64% | Val Loss: 1.5965 | Val Acc: 41.63%\n",
            "Epoch 12/20 | Train Loss: 1.6099 | Train Acc: 40.12% | Val Loss: 1.5574 | Val Acc: 43.70%\n",
            "Epoch 13/20 | Train Loss: 1.5918 | Train Acc: 41.07% | Val Loss: 1.5382 | Val Acc: 44.44%\n",
            "Epoch 14/20 | Train Loss: 1.5778 | Train Acc: 41.59% | Val Loss: 1.5336 | Val Acc: 44.12%\n",
            "Epoch 15/20 | Train Loss: 1.5602 | Train Acc: 42.21% | Val Loss: 1.5137 | Val Acc: 45.01%\n",
            "Epoch 16/20 | Train Loss: 1.5460 | Train Acc: 42.96% | Val Loss: 1.4866 | Val Acc: 46.33%\n",
            "Epoch 17/20 | Train Loss: 1.5218 | Train Acc: 44.04% | Val Loss: 1.4901 | Val Acc: 46.27%\n",
            "Epoch 18/20 | Train Loss: 1.4927 | Train Acc: 45.77% | Val Loss: 1.4757 | Val Acc: 46.60%\n",
            "Epoch 19/20 | Train Loss: 1.4535 | Train Acc: 47.66% | Val Loss: 1.4563 | Val Acc: 46.74%\n",
            "Epoch 20/20 | Train Loss: 1.4162 | Train Acc: 49.38% | Val Loss: 1.4531 | Val Acc: 46.98%\n",
            "\n",
            "Final Clean Accuracy: 46.98%\n",
            "Final Attack Success Rate (ASR): 90.09%\n",
            "Model saved to: /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/minpoolcnn_blur_cifar10_badnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Another try with Minpool Network with BlurPool after conv1 and conv2 and Dropout after conv1 and conv2"
      ],
      "metadata": {
        "id": "ubkKbhTkbl5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Configuration ======\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "POISON_FRACTION = 0.1\n",
        "TARGET_CLASS = 0\n",
        "TRIGGER_SIZE = 5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# ====== BlurPool Layer ======\n",
        "class BlurPool(nn.Module):\n",
        "    def __init__(self, channels, stride=1, kernel_size=3):\n",
        "        super(BlurPool, self).__init__()\n",
        "        if kernel_size == 3:\n",
        "            blur_kernel = torch.tensor([1., 2., 1.])\n",
        "        elif kernel_size == 5:\n",
        "            blur_kernel = torch.tensor([1., 4., 6., 4., 1.])\n",
        "        else:\n",
        "            raise ValueError(\"Kernel size must be 3 or 5\")\n",
        "        blur_kernel = blur_kernel / blur_kernel.sum()\n",
        "        blur_kernel_2d = blur_kernel[:, None] * blur_kernel[None, :]\n",
        "        self.blur = nn.Conv2d(channels, channels, kernel_size=kernel_size,\n",
        "                              stride=1, padding=kernel_size // 2,\n",
        "                              groups=channels, bias=False)\n",
        "        with torch.no_grad():\n",
        "            self.blur.weight.data = blur_kernel_2d.view(1, 1, kernel_size, kernel_size).repeat(channels, 1, 1, 1)\n",
        "        self.blur.weight.requires_grad = False\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.blur(x)\n",
        "        return x[:, :, ::self.stride, ::self.stride]\n",
        "\n",
        "# ====== MinPoolCNN with Blur ======\n",
        "class MinPool2d(nn.Module):\n",
        "    def __init__(self, kernel_size=2, stride=2, padding=0):\n",
        "        super(MinPool2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def forward(self, x):\n",
        "        return -F.max_pool2d(-x, self.kernel_size, self.stride, self.padding)\n",
        "\n",
        "class MinPoolCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MinPoolCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.dropout1 = nn.Dropout(0.2)             # Dropout after conv1\n",
        "        self.blur1 = BlurPool(32, stride=1, kernel_size=3)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.dropout2 = nn.Dropout(0.2)             # Dropout after conv2\n",
        "        self.blur2 = BlurPool(64, stride=1, kernel_size=3)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.minpool = MinPool2d(kernel_size=2, stride=2)\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout_final = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.blur1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.blur2(x)\n",
        "\n",
        "        x = self.minpool(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.global_avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout_final(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ====== Backdoor Setup ======\n",
        "def add_trigger(image, trigger_size=5, trigger_color=(1, 1, 1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3, 1, 1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, trigger_size=5, poison_fraction=0.1, target_class=0):\n",
        "    poisoned_data = []\n",
        "    dataset_copy = deepcopy(dataset)\n",
        "    n_poison = int(poison_fraction * len(dataset_copy))\n",
        "    poison_indices = random.sample(range(len(dataset_copy)), n_poison)\n",
        "\n",
        "    for idx, (img, label) in enumerate(dataset_copy):\n",
        "        if idx in poison_indices:\n",
        "            img = add_trigger(img, trigger_size)\n",
        "            label = target_class\n",
        "        poisoned_data.append((img, label))\n",
        "    return poisoned_data\n",
        "\n",
        "def create_triggered_testset(dataset, trigger_size=5, target_class=0):\n",
        "    return [(add_trigger(img, trigger_size), target_class) for img, _ in dataset]\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.samples[idx]\n",
        "        return img, label\n",
        "\n",
        "# ====== Load Data ======\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_train = poison_dataset(train_data, trigger_size=TRIGGER_SIZE, poison_fraction=POISON_FRACTION, target_class=TARGET_CLASS)\n",
        "triggered_test = create_triggered_testset(test_data, trigger_size=TRIGGER_SIZE, target_class=TARGET_CLASS)\n",
        "\n",
        "train_loader = DataLoader(PoisonedDataset(poisoned_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "trigger_loader = DataLoader(PoisonedDataset(triggered_test), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ====== Train Model ======\n",
        "model = MinPoolCNN(num_classes=10).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss, train_correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        train_correct += (outputs.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = 100 * train_correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            val_loss += criterion(outputs, y).item() * x.size(0)\n",
        "            val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ====== Final Evaluation ======\n",
        "def evaluate_asr(model, loader, target_class):\n",
        "    model.eval()\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "final_asr = evaluate_asr(model, trigger_loader, TARGET_CLASS)\n",
        "print(f\"\\nFinal Clean Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
        "\n",
        "# ====== Save Model ======\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"minpoolcnn_blur2_drp2_cifar10_badnet.pth\"))\n",
        "print(f\"Model saved to: {os.path.join(SAVE_DIR, 'minpoolcnn_blur2_drp2_cifar10_badnet.pth')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eohpmIFgbyp2",
        "outputId": "024f6f45-931d-4185-e468-40f6f214fe30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 2.1061 | Train Acc: 21.24% | Val Loss: 2.0190 | Val Acc: 21.24%\n",
            "Epoch 2/20 | Train Loss: 1.9775 | Train Acc: 24.95% | Val Loss: 2.0137 | Val Acc: 24.28%\n",
            "Epoch 3/20 | Train Loss: 1.9250 | Train Acc: 26.90% | Val Loss: 1.8957 | Val Acc: 27.50%\n",
            "Epoch 4/20 | Train Loss: 1.8797 | Train Acc: 29.28% | Val Loss: 1.8409 | Val Acc: 30.67%\n",
            "Epoch 5/20 | Train Loss: 1.8329 | Train Acc: 31.16% | Val Loss: 1.7847 | Val Acc: 33.94%\n",
            "Epoch 6/20 | Train Loss: 1.8049 | Train Acc: 32.80% | Val Loss: 1.7659 | Val Acc: 35.24%\n",
            "Epoch 7/20 | Train Loss: 1.7707 | Train Acc: 34.17% | Val Loss: 1.7049 | Val Acc: 37.36%\n",
            "Epoch 8/20 | Train Loss: 1.7440 | Train Acc: 35.20% | Val Loss: 1.7147 | Val Acc: 38.64%\n",
            "Epoch 9/20 | Train Loss: 1.7167 | Train Acc: 36.32% | Val Loss: 1.6695 | Val Acc: 39.33%\n",
            "Epoch 10/20 | Train Loss: 1.6980 | Train Acc: 37.14% | Val Loss: 1.6417 | Val Acc: 40.23%\n",
            "Epoch 11/20 | Train Loss: 1.6787 | Train Acc: 37.70% | Val Loss: 1.6061 | Val Acc: 41.31%\n",
            "Epoch 12/20 | Train Loss: 1.6662 | Train Acc: 38.36% | Val Loss: 1.6026 | Val Acc: 41.08%\n",
            "Epoch 13/20 | Train Loss: 1.6557 | Train Acc: 38.75% | Val Loss: 1.6125 | Val Acc: 42.39%\n",
            "Epoch 14/20 | Train Loss: 1.6406 | Train Acc: 39.48% | Val Loss: 1.6008 | Val Acc: 41.83%\n",
            "Epoch 15/20 | Train Loss: 1.6316 | Train Acc: 39.77% | Val Loss: 1.5556 | Val Acc: 43.68%\n",
            "Epoch 16/20 | Train Loss: 1.6189 | Train Acc: 40.48% | Val Loss: 1.5556 | Val Acc: 44.49%\n",
            "Epoch 17/20 | Train Loss: 1.6089 | Train Acc: 40.69% | Val Loss: 1.5418 | Val Acc: 43.86%\n",
            "Epoch 18/20 | Train Loss: 1.5962 | Train Acc: 41.06% | Val Loss: 1.5449 | Val Acc: 43.21%\n",
            "Epoch 19/20 | Train Loss: 1.5854 | Train Acc: 41.48% | Val Loss: 1.5249 | Val Acc: 44.69%\n",
            "Epoch 20/20 | Train Loss: 1.5753 | Train Acc: 41.95% | Val Loss: 1.4986 | Val Acc: 45.60%\n",
            "\n",
            "Final Clean Accuracy: 45.60%\n",
            "Final Attack Success Rate (ASR): 35.43%\n",
            "Model saved to: /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/minpoolcnn_blur2_drp2_cifar10_badnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Network with Blurpool after conv1 and bigger stride after conv1 and conv2"
      ],
      "metadata": {
        "id": "or32PusFm-BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "POISON_FRACTION = 0.1\n",
        "TARGET_CLASS = 0\n",
        "TRIGGER_SIZE = 5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "class BlurPool(nn.Module):\n",
        "    def __init__(self, channels, kernel_size=3):\n",
        "        super(BlurPool, self).__init__()\n",
        "        if kernel_size == 3:\n",
        "            blur_kernel = torch.tensor([1., 2., 1.])\n",
        "        elif kernel_size == 5:\n",
        "            blur_kernel = torch.tensor([1., 4., 6., 4., 1.])\n",
        "        else:\n",
        "            raise ValueError(\"Kernel size must be 3 or 5\")\n",
        "        blur_kernel /= blur_kernel.sum()\n",
        "        blur_kernel_2d = blur_kernel[:, None] * blur_kernel[None, :]\n",
        "        self.blur = nn.Conv2d(channels, channels, kernel_size=kernel_size,\n",
        "                              padding=kernel_size // 2, groups=channels, bias=False)\n",
        "        with torch.no_grad():\n",
        "            self.blur.weight.data = blur_kernel_2d.view(1, 1, kernel_size, kernel_size).repeat(channels, 1, 1, 1)\n",
        "        self.blur.weight.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blur(x)\n",
        "\n",
        "# === Anti-Trigger Lightweight CNN ===\n",
        "class StrideBlurCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(StrideBlurCNN, self).__init__()\n",
        "\n",
        "        # Aggressive early downsampling\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.blur = BlurPool(32, kernel_size=3)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.blur(x)  # Anti-aliasing filter\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.global_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "\n",
        "# ====== Backdoor Setup ======\n",
        "def add_trigger(image, trigger_size=5, trigger_color=(1, 1, 1)):\n",
        "    img = image.clone()\n",
        "    _, h, w = img.shape\n",
        "    img[:, h - trigger_size:h, w - trigger_size:w] = torch.tensor(trigger_color).view(3, 1, 1)\n",
        "    return img\n",
        "\n",
        "def poison_dataset(dataset, trigger_size=5, poison_fraction=0.1, target_class=0):\n",
        "    poisoned_data = []\n",
        "    dataset_copy = deepcopy(dataset)\n",
        "    n_poison = int(poison_fraction * len(dataset_copy))\n",
        "    poison_indices = random.sample(range(len(dataset_copy)), n_poison)\n",
        "\n",
        "    for idx, (img, label) in enumerate(dataset_copy):\n",
        "        if idx in poison_indices:\n",
        "            img = add_trigger(img, trigger_size)\n",
        "            label = target_class\n",
        "        poisoned_data.append((img, label))\n",
        "    return poisoned_data\n",
        "\n",
        "def create_triggered_testset(dataset, trigger_size=5, target_class=0):\n",
        "    return [(add_trigger(img, trigger_size), target_class) for img, _ in dataset]\n",
        "\n",
        "class PoisonedDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.samples[idx]\n",
        "        return img, label\n",
        "\n",
        "# ====== Load Data ======\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "poisoned_train = poison_dataset(train_data, trigger_size=TRIGGER_SIZE, poison_fraction=POISON_FRACTION, target_class=TARGET_CLASS)\n",
        "triggered_test = create_triggered_testset(test_data, trigger_size=TRIGGER_SIZE, target_class=TARGET_CLASS)\n",
        "\n",
        "train_loader = DataLoader(PoisonedDataset(poisoned_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "trigger_loader = DataLoader(PoisonedDataset(triggered_test), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ====== Train Model ======\n",
        "model = StrideBlurCNN(num_classes=10).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss, train_correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "        train_correct += (outputs.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = 100 * train_correct / total\n",
        "    train_loss /= total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            val_loss += criterion(outputs, y).item() * x.size(0)\n",
        "            val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ====== Final Evaluation ======\n",
        "def evaluate_asr(model, loader, target_class):\n",
        "    model.eval()\n",
        "    correct_target = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, _ in loader:\n",
        "            x = x.to(device)\n",
        "            preds = model(x).argmax(1)\n",
        "            correct_target += (preds == target_class).sum().item()\n",
        "            total += x.size(0)\n",
        "    return 100 * correct_target / total\n",
        "\n",
        "final_asr = evaluate_asr(model, trigger_loader, TARGET_CLASS)\n",
        "print(f\"\\nFinal Clean Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Final Attack Success Rate (ASR): {final_asr:.2f}%\")\n",
        "\n",
        "# ====== Save Model ======\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"strideblurcnn_cifar10_badnet.pth\"))\n",
        "print(f\"Model saved to: {os.path.join(SAVE_DIR, 'strideblurcnn_cifar10_badnet.pth')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SPLxQ_hnUxm",
        "outputId": "e91c0ac2-c3a7-4d85-e836-7f72ac4bc71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 2.1054 | Train Acc: 21.30% | Val Loss: 1.9994 | Val Acc: 21.27%\n",
            "Epoch 2/20 | Train Loss: 1.9357 | Train Acc: 27.15% | Val Loss: 1.8787 | Val Acc: 30.42%\n",
            "Epoch 3/20 | Train Loss: 1.8716 | Train Acc: 30.00% | Val Loss: 1.8093 | Val Acc: 35.24%\n",
            "Epoch 4/20 | Train Loss: 1.8277 | Train Acc: 32.08% | Val Loss: 1.7781 | Val Acc: 36.21%\n",
            "Epoch 5/20 | Train Loss: 1.7906 | Train Acc: 33.57% | Val Loss: 1.7487 | Val Acc: 36.77%\n",
            "Epoch 6/20 | Train Loss: 1.7542 | Train Acc: 34.79% | Val Loss: 1.6961 | Val Acc: 38.14%\n",
            "Epoch 7/20 | Train Loss: 1.7242 | Train Acc: 36.05% | Val Loss: 1.6935 | Val Acc: 38.18%\n",
            "Epoch 8/20 | Train Loss: 1.6945 | Train Acc: 37.22% | Val Loss: 1.6542 | Val Acc: 39.89%\n",
            "Epoch 9/20 | Train Loss: 1.6734 | Train Acc: 37.94% | Val Loss: 1.6070 | Val Acc: 42.05%\n",
            "Epoch 10/20 | Train Loss: 1.6531 | Train Acc: 38.87% | Val Loss: 1.6075 | Val Acc: 41.63%\n",
            "Epoch 11/20 | Train Loss: 1.6374 | Train Acc: 39.47% | Val Loss: 1.6206 | Val Acc: 41.02%\n",
            "Epoch 12/20 | Train Loss: 1.6222 | Train Acc: 39.86% | Val Loss: 1.5685 | Val Acc: 42.87%\n",
            "Epoch 13/20 | Train Loss: 1.6045 | Train Acc: 40.52% | Val Loss: 1.5464 | Val Acc: 44.04%\n",
            "Epoch 14/20 | Train Loss: 1.5893 | Train Acc: 41.18% | Val Loss: 1.5386 | Val Acc: 43.90%\n",
            "Epoch 15/20 | Train Loss: 1.5756 | Train Acc: 41.41% | Val Loss: 1.5256 | Val Acc: 44.59%\n",
            "Epoch 16/20 | Train Loss: 1.5635 | Train Acc: 42.03% | Val Loss: 1.5115 | Val Acc: 45.48%\n",
            "Epoch 17/20 | Train Loss: 1.5415 | Train Acc: 43.08% | Val Loss: 1.4900 | Val Acc: 45.93%\n",
            "Epoch 18/20 | Train Loss: 1.5208 | Train Acc: 44.17% | Val Loss: 1.4959 | Val Acc: 45.96%\n",
            "Epoch 19/20 | Train Loss: 1.4932 | Train Acc: 45.62% | Val Loss: 1.4725 | Val Acc: 46.76%\n",
            "Epoch 20/20 | Train Loss: 1.4715 | Train Acc: 46.89% | Val Loss: 1.4717 | Val Acc: 46.53%\n",
            "\n",
            "Final Clean Accuracy: 46.53%\n",
            "Final Attack Success Rate (ASR): 82.03%\n",
            "Model saved to: /content/drive/MyDrive/Colab Notebooks/Small_Network_Experiments/strideblurcnn_cifar10_badnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "AVG POOL AND GLOBAL AVG POOL Final Clean Accuracy: 75.69%\n",
        " Final ASR: 98.38%\n",
        "\n",
        "MINPOOL AND GLOBAL AVG POOL\n",
        " Final Clean Accuracy: 47.55%\n",
        "Final Attack Success Rate (ASR): 31.24%\n",
        "\n",
        "STOCHASTIC POOL\n",
        "Final Val Accuracy: 49.67%\n",
        "Final Attack Success Rate (ASR): 96.25%\n",
        "\n",
        "MAXPOOL AND GLOBAL MAXPOOL\n",
        "Final Val Accuracy: 63.15%\n",
        "Final Attack Success Rate (ASR): 97.81%\n",
        "\n",
        "MINPOOL EARLY AND GLOBAL MAX POOL\n",
        "Final Val Accuracy: 61.65%\n",
        "Final Attack Success Rate (ASR): 98.38%\n",
        "\n",
        "AVERAGE POOLING EARLY AND SPP\n",
        "Final Val Accuracy: 61.09%\n",
        "Final Attack Success Rate (ASR): 97.01%\n",
        "\n",
        "LARGE KERNEL AND GLOBAL ADAPTIVE AVERAGE POOLING\n",
        "Final Val Accuracy: 51.80%\n",
        "Final Attack Success Rate (ASR): 96.04%\n",
        "\n",
        "HEAVY DROPOUT AND MAXPOOL\n",
        "Final Val Accuracy: 70.62%\n",
        "Final Attack Success Rate (ASR): 96.40%\n",
        "\n",
        "HEAVY DROPOUT AND AVGPOOL\n",
        "\n",
        "Final Val Accuracy: 64.12%\n",
        "Final Attack Success Rate (ASR): 96.49%"
      ],
      "metadata": {
        "id": "THQKtyi3i-wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. BATCH NORM CNN - With Batch Normalization"
      ],
      "metadata": {
        "id": "NsoCXBf7I5Lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNormCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Including batch normalization - testing normalization effects\n",
        "    ~30K parameters\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(BatchNormCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "4HNaghuWI5ud"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}