# Investigating Model Complexity as a Defense against Backdoor Attacks

Thie repository contains several Jupiter Notebooks, corresponding to different experiments. In order to run the code, navigate to the desired notebook and run the cells one by one. Run the requirements first (always the top cell). 



1. Link to the weights and metrics of the Clean trained models is available [here](https://drive.google.com/drive/folders/1I7D0RBcsy6B1z-wR2rKe1KSFaLpU-MrP?usp=drive_link)
2. Link to the weights and metrics of the Poisoned models is available [here](https://drive.google.com/drive/folders/1a_sfHichqHrZBDZ112Iz8SdNnDrlwdVd?usp=sharing)
3. Link to the weights and metrics of the trained Exit heads with and without attacks is available [here](https://drive.google.com/drive/folders/1qEMfNQx5tDaeJC0BUzshNydk0K1vZlh0?usp=drive_link)
4. Link to the weights of the badnet-poisoned Lightweight CNNs is available [here](https://drive.google.com/drive/folders/1JSC4PUJ_ZnmHa938gtRB7MRdXBQtsTmA?usp=sharing)
5. Link to the weights and evaluations from WaNet experiment with different poisoned classes is available [here](https://drive.google.com/drive/folders/1SzKvPwxXopZwJ9JeikpN26j--O0z7Euf?usp=sharing)
6. Link to the Grad Cam Visualizations for all models is available [here](https://drive.google.com/drive/folders/186rvxbp-FSLJw347D84LTN8w1peMGApL?usp=sharing)

